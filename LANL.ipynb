{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LANL Earthquake Prediction Kaggle Competition 2019\n",
    "### Eric Yap, Joel Huang, Kyra Wang\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we present our work for the LANL Earthquake Prediction Kaggle Competition 2019. The goal of this competition is to use seismic signals to predict the timing of laboratory earthquakes. The data comes from a well-known experimental set-up used to study earthquake physics. The `acoustic_data` input signal is used to predict the time remaining before the next laboratory earthquake (`time_to_failure`).\n",
    "\n",
    "The training data is a single, continuous segment of experimental data. The test data consists of a folder containing many small segments. The data within each test file is continuous, but the test files do not represent a continuous segment of the experiment; thus, the predictions cannot be assumed to follow the same regular pattern seen in the training file.\n",
    "\n",
    "For each `seg_id` in the test folder, we need to predict a single `time_to_failure` corresponding to the time between the last row of the segment and the next laboratory earthquake.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Data wrangling imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Utility imports\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Data visualization imports\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as tick\n",
    "import seaborn as sns\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "\n",
    "# Custom stuff\n",
    "from data import LANLDataset, FeatureGenerator\n",
    "\n",
    "# Setting the seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "else:\n",
    "    torch.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "As the training data and the test data are formatted differently, we must either preprocess the data such that the formats of both sets are the same, or ensure that our model is capable of predicting on the two different formats. We went with the first option because it is less time consuming to implement.\n",
    "\n",
    "We did this by splitting the training data into segments the same size as the test data segments, i.e. 150000 data points each. Each segment is labeled with a single `time_to_failure` corresponding to the time between the last row of the segment and the next laboratory earthquake. We then put each of these segments into a single dataframe, and saved this as a pickle file to be used as our training data.\n",
    "\n",
    "Following this, we merged the separate test segments into another single dataframe, and saved this as a pickle file to be used as our test data.\n",
    "\n",
    "As the dataset is massive, we used Joblib to help run the functions as a pipeline jobs with parallel computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seg_id</th>\n",
       "      <th>segment</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>[12, 6, 8, 5, 8, 8, 9, 7, -5, 3, 5, 2, 2, 3, -...</td>\n",
       "      <td>1.430797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>[5, 6, 8, 6, 3, -1, 5, 4, 4, 4, 6, 5, 5, 5, 6,...</td>\n",
       "      <td>1.391499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>[5, 5, 8, 9, 9, 10, 11, 12, 13, 5, 3, 7, 5, 3,...</td>\n",
       "      <td>1.353196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>[5, -5, -4, 1, 3, 4, 6, 12, 15, 17, 14, 9, 6, ...</td>\n",
       "      <td>1.313798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>[12, 6, 4, -1, 0, 6, 7, 6, 2, -2, 0, 4, 1, 5, ...</td>\n",
       "      <td>1.274400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    seg_id                                            segment    target\n",
       "0  train_0  [12, 6, 8, 5, 8, 8, 9, 7, -5, 3, 5, 2, 2, 3, -...  1.430797\n",
       "1  train_1  [5, 6, 8, 6, 3, -1, 5, 4, 4, 4, 6, 5, 5, 5, 6,...  1.391499\n",
       "2  train_2  [5, 5, 8, 9, 9, 10, 11, 12, 13, 5, 3, 7, 5, 3,...  1.353196\n",
       "3  train_3  [5, -5, -4, 1, 3, 4, 6, 12, 15, 17, 14, 9, 6, ...  1.313798\n",
       "4  train_4  [12, 6, 4, -1, 0, 6, 7, 6, 2, -2, 0, 4, 1, 5, ...  1.274400"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainval_df = pd.read_pickle('./data/train_features.pkl')\n",
    "trainval_df = trainval_df[:-2]\n",
    "trainval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seg_id</th>\n",
       "      <th>segment</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>seg_00030f</td>\n",
       "      <td>[4, 0, -2, 0, 2, -3, -9, -4, 11, 11, 8, 1, 10,...</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>seg_0012b5</td>\n",
       "      <td>[5, 8, 8, 7, 4, 1, -1, -4, -1, 0, 5, 7, -1, 7,...</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>seg_00184e</td>\n",
       "      <td>[8, 2, 3, 8, 7, 9, 7, 4, 4, 9, 9, 1, 2, 6, 4, ...</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>seg_003339</td>\n",
       "      <td>[2, 6, 3, 6, 8, 6, 8, 5, 4, 6, 2, 3, 1, 4, 6, ...</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>seg_0042cc</td>\n",
       "      <td>[5, 3, 1, 4, 6, 6, 7, 4, 5, 4, 3, 4, 6, 7, 3, ...</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       seg_id                                            segment  target\n",
       "0  seg_00030f  [4, 0, -2, 0, 2, -3, -9, -4, 11, 11, 8, 1, 10,...    -999\n",
       "1  seg_0012b5  [5, 8, 8, 7, 4, 1, -1, -4, -1, 0, 5, 7, -1, 7,...    -999\n",
       "2  seg_00184e  [8, 2, 3, 8, 7, 9, 7, 4, 4, 9, 9, 1, 2, 6, 4, ...    -999\n",
       "3  seg_003339  [2, 6, 3, 6, 8, 6, 8, 5, 4, 6, 2, 3, 1, 4, 6, ...    -999\n",
       "4  seg_0042cc  [5, 3, 1, 4, 6, 6, 7, 4, 5, 4, 3, 4, 6, 7, 3, ...    -999"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_pickle('./data/test_features.pkl')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we split the training data further into a 80/20 training/validation split. We then create dataloaders that will help load the data into the model in parallel using multiprocessing workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 8\n",
    "params = {'batch_size': batch_size,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 16}\n",
    "\n",
    "def get_df_features(start_index, end_index, chunk_size, mode=\"trainval\"):\n",
    "    feats = []\n",
    "    for i in tqdm(range(len(trainval_df))):\n",
    "        tv_seg = np.asarray(trainval_df[\"segment\"][i])\n",
    "        tmp = []\n",
    "        for j in range(start_index, end_index, chunk_size):\n",
    "            x = pd.Series(tv_seg[j:j+chunk_size])\n",
    "            tmp.append([\n",
    "                x.mean(), # mean\n",
    "                x.std(), # std\n",
    "                x.max(), # max\n",
    "                x.min(), # min\n",
    "                np.mean(np.diff(x)), # mean change abs\n",
    "                np.abs(x).max(), # abs max\n",
    "                np.abs(x).min(), # abs min\n",
    "                x.max() / np.abs(x.min()), # max to min\n",
    "                x.max() - np.abs(x.min()), # max to min diff\n",
    "                len(x[np.abs(x) > 500]), # count big\n",
    "                x.sum(), # sum\n",
    "                np.quantile(x, 0.95), # q95\n",
    "                np.quantile(x, 0.99), # q99\n",
    "                np.quantile(x, 0.05), # q05\n",
    "                np.quantile(x, 0.01), # q01\n",
    "                np.quantile(np.abs(x), 0.95), # abs q95\n",
    "                np.quantile(np.abs(x), 0.99), # abs q99\n",
    "                np.quantile(np.abs(x), 0.05), # abs q05\n",
    "                np.quantile(np.abs(x), 0.01), # abs q01\n",
    "                np.abs(x).mean(), # abs mean\n",
    "                np.abs(x).std(), # abs std\n",
    "                x.mad(), # mad\n",
    "                x.kurtosis(), # kurt\n",
    "                x.skew(), # skew\n",
    "                x.median(), # med\n",
    "                x.sum() # sum\n",
    "            ])\n",
    "        feats.append(tmp)\n",
    "    np.save(f\"./data/{mode}_features15.npy\", np.asarray(feats))\n",
    "    return feats\n",
    "\n",
    "seg = []\n",
    "#trainval_feats = get_df_features(0, 150000, 10000)\n",
    "#test_feats = get_df_features(0, 150000, 10000)\n",
    "trainval_feats = np.load(\"./data/trainval_features15.npy\").tolist()\n",
    "\n",
    "train_val = LANLDataset(trainval_feats, trainval_df[\"target\"].to_numpy())\n",
    "#test = LANLDataset(test_feats, test_df[\"target\"].to_numpy())\n",
    "train, val = train_val.train_val_split(0.7, 0.3)\n",
    "\n",
    "datasets = {'train': train,\n",
    "            'valid': val}\n",
    "            #'test' : test}\n",
    "dataloaders = {phase: data.DataLoader(dataset, **params)\n",
    "               for phase, dataset in datasets.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LANLModel(nn.Module):\n",
    "    def __init__(self, device, input_dim=1, hidden_dim=64, output_dim=1, batch_size=64, num_layers=1):\n",
    "        super(LANLModel, self).__init__()\n",
    "        self.device = device\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (\n",
    "            torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).to(self.device),\n",
    "            torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).to(self.device)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = x.unsqueeze(2)\n",
    "        rnn_out, _ = self.rnn(x, self.init_hidden())\n",
    "        out = self.linear(rnn_out[:,-1,:])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "3.587816712634811\n",
      "Epoch 1\n",
      "3.041500401301462\n",
      "Epoch 2\n",
      "3.0423623551436463\n",
      "Epoch 3\n",
      "3.040751801488178\n",
      "Epoch 4\n",
      "3.0450742899394427\n",
      "Epoch 5\n",
      "3.0435435052126483\n",
      "Epoch 6\n",
      "3.043567792313998\n",
      "Epoch 7\n",
      "3.039339923142084\n",
      "Epoch 8\n",
      "3.043522341655252\n",
      "Epoch 9\n",
      "3.040382200902928\n",
      "Epoch 10\n",
      "3.0436629157900157\n",
      "Epoch 11\n",
      "3.0410529417418393\n",
      "Epoch 12\n",
      "3.0451078056637706\n",
      "Epoch 13\n",
      "3.044564887148435\n",
      "Epoch 14\n",
      "3.0387568249077095\n",
      "Epoch 15\n",
      "3.0377392078357968\n",
      "Epoch 16\n",
      "3.043285914783269\n",
      "Epoch 17\n",
      "3.04161457285855\n",
      "Epoch 18\n",
      "3.041993435940456\n",
      "Epoch 19\n",
      "3.035007687865711\n",
      "Epoch 20\n",
      "3.047264357082179\n",
      "Epoch 21\n",
      "3.0433561740025796\n",
      "Epoch 22\n",
      "3.0418013093250047\n",
      "Epoch 23\n",
      "3.039602721486587\n",
      "Epoch 24\n",
      "3.0443828666145034\n",
      "Epoch 25\n",
      "3.0426980260291385\n",
      "Epoch 26\n",
      "3.044612945266109\n",
      "Epoch 27\n",
      "3.041947224101082\n",
      "Epoch 28\n",
      "3.0437127869637286\n",
      "Epoch 29\n",
      "3.040161385887959\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = LANLModel(device, input_dim=26, hidden_dim=100, num_layers=2, batch_size=batch_size)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "def train():\n",
    "    model.train(mode=True)\n",
    "    for epoch in range(30):\n",
    "        print(\"Epoch \" + str(epoch))\n",
    "        train_loss = 0\n",
    "        for idx, sample in enumerate(dataloaders[\"train\"]):\n",
    "            if idx == len(dataloaders[\"train\"]) - 1:\n",
    "                continue\n",
    "            data, targets = sample[\"data\"].to(device), sample[\"target\"].to(device)\n",
    "            model.zero_grad()\n",
    "            outputs = model(data)\n",
    "            #print(outputs)\n",
    "            loss = criterion(outputs.float(), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= (len(dataloaders[\"train\"]) - 1)\n",
    "        print(train_loss)\n",
    "        \n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model on the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LANL_test():\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.shape for x in model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
