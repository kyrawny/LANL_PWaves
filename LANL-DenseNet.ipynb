{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LANL Earthquake Prediction Kaggle Competition 2019\n",
    "### Eric Yap, Joel Huang, Kyra Wang\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we present our work for the LANL Earthquake Prediction Kaggle Competition 2019. The goal of this competition is to use seismic signals to predict the timing of laboratory earthquakes. The data comes from a well-known experimental set-up used to study earthquake physics. The `acoustic_data` input signal is used to predict the time remaining before the next laboratory earthquake (`time_to_failure`).\n",
    "\n",
    "The training data is a single, continuous segment of experimental data. The test data consists of a folder containing many small segments. The data within each test file is continuous, but the test files do not represent a continuous segment of the experiment; thus, the predictions cannot be assumed to follow the same regular pattern seen in the training file.\n",
    "\n",
    "For each `seg_id` in the test folder, we need to predict a single `time_to_failure` corresponding to the time between the last row of the segment and the next laboratory earthquake.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, absolute_import\n",
    "\n",
    "# Data wrangling imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Utility imports\n",
    "import os\n",
    "import os.path\n",
    "import time\n",
    "import math\n",
    "import ast\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Data visualization imports\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as tick\n",
    "import seaborn as sns\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch.nn.utils import weight_norm\n",
    "from torch.autograd import Variable, Function\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.utils.checkpoint as cp\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "# Our own utility code\n",
    "from utils.lr_finder import LRFinder\n",
    "from utils.lanl_data import LANL_FeatureGenerator, LANL_Dataset, LANL_Dataset_LR\n",
    "from utils.adamw import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla P4\n"
     ]
    }
   ],
   "source": [
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "print(torch.cuda.get_device_name(device))\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Setting the seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "else:\n",
    "    torch.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "As the training data and the test data are formatted differently, we must either preprocess the data such that the formats of both sets are the same, or ensure that our model is capable of predicting on the two different formats. We went with the first option because it is less time consuming to implement.\n",
    "\n",
    "We did this by splitting the training data into segments the same size as the test data segments, i.e. 150000 data points each. Each segment is labeled with a single `time_to_failure` corresponding to the time between the last row of the segment and the next laboratory earthquake. We then put each of these segments into a single dataframe, and saved this as a pickle file to be used as our training data.\n",
    "\n",
    "Following this, we merged the separate test segments into another single dataframe, and saved this as a pickle file to be used as our test data.\n",
    "\n",
    "As the dataset is massive, we used `Joblib` to help run the functions as a pipeline jobs with parallel computing. The feature extraction code is in `./utils/lanl_data.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kaggle\n",
    "# !mkdir ./data\n",
    "# !kaggle competitions download -p ./data -c LANL-Earthquake-Prediction\n",
    "\n",
    "# !unzip ./data/train.csv.zip -d ./data\n",
    "# !mkdir ./data/test\n",
    "# !unzip ./data/test.zip -d ./data/test\n",
    "\n",
    "# !chmod 644 ./data/train.csv\n",
    "# !find ./data/test -type f -exec chmod 644 {} \\;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4195it [02:12, 31.72it/s]                          \n",
      "100%|██████████| 2624/2624 [00:30<00:00, 84.69it/s]\n"
     ]
    }
   ],
   "source": [
    "training_fg = LANL_FeatureGenerator(dtype='train', n_jobs=16, chunk_size=150000)\n",
    "training_data = training_fg.generate()\n",
    "\n",
    "test_fg = LANL_FeatureGenerator(dtype='test', n_jobs=16, chunk_size=None)\n",
    "test_data = test_fg.generate()\n",
    "\n",
    "training_data.to_pickle('./data/train_features.pkl')\n",
    "test_data.to_pickle('./data/test_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seg_id</th>\n",
       "      <th>segment</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>[12, 6, 8, 5, 8, 8, 9, 7, -5, 3, 5, 2, 2, 3, -...</td>\n",
       "      <td>1.430797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>[5, 6, 8, 6, 3, -1, 5, 4, 4, 4, 6, 5, 5, 5, 6,...</td>\n",
       "      <td>1.391499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>[5, 5, 8, 9, 9, 10, 11, 12, 13, 5, 3, 7, 5, 3,...</td>\n",
       "      <td>1.353196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>[5, -5, -4, 1, 3, 4, 6, 12, 15, 17, 14, 9, 6, ...</td>\n",
       "      <td>1.313798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>[12, 6, 4, -1, 0, 6, 7, 6, 2, -2, 0, 4, 1, 5, ...</td>\n",
       "      <td>1.274400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    seg_id                                            segment    target\n",
       "0  train_0  [12, 6, 8, 5, 8, 8, 9, 7, -5, 3, 5, 2, 2, 3, -...  1.430797\n",
       "1  train_1  [5, 6, 8, 6, 3, -1, 5, 4, 4, 4, 6, 5, 5, 5, 6,...  1.391499\n",
       "2  train_2  [5, 5, 8, 9, 9, 10, 11, 12, 13, 5, 3, 7, 5, 3,...  1.353196\n",
       "3  train_3  [5, -5, -4, 1, 3, 4, 6, 12, 15, 17, 14, 9, 6, ...  1.313798\n",
       "4  train_4  [12, 6, 4, -1, 0, 6, 7, 6, 2, -2, 0, 4, 1, 5, ...  1.274400"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainval_df = pd.read_pickle('./data/train_features.pkl')\n",
    "trainval_df = trainval_df[trainval_df.segment.map(len) == 150000]\n",
    "\n",
    "trainval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seg_id</th>\n",
       "      <th>segment</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>seg_00030f</td>\n",
       "      <td>[4, 0, -2, 0, 2, -3, -9, -4, 11, 11, 8, 1, 10,...</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>seg_0012b5</td>\n",
       "      <td>[5, 8, 8, 7, 4, 1, -1, -4, -1, 0, 5, 7, -1, 7,...</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>seg_00184e</td>\n",
       "      <td>[8, 2, 3, 8, 7, 9, 7, 4, 4, 9, 9, 1, 2, 6, 4, ...</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>seg_003339</td>\n",
       "      <td>[2, 6, 3, 6, 8, 6, 8, 5, 4, 6, 2, 3, 1, 4, 6, ...</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>seg_0042cc</td>\n",
       "      <td>[5, 3, 1, 4, 6, 6, 7, 4, 5, 4, 3, 4, 6, 7, 3, ...</td>\n",
       "      <td>-999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       seg_id                                            segment  target\n",
       "0  seg_00030f  [4, 0, -2, 0, 2, -3, -9, -4, 11, 11, 8, 1, 10,...    -999\n",
       "1  seg_0012b5  [5, 8, 8, 7, 4, 1, -1, -4, -1, 0, 5, 7, -1, 7,...    -999\n",
       "2  seg_00184e  [8, 2, 3, 8, 7, 9, 7, 4, 4, 9, 9, 1, 2, 6, 4, ...    -999\n",
       "3  seg_003339  [2, 6, 3, 6, 8, 6, 8, 5, 4, 6, 2, 3, 1, 4, 6, ...    -999\n",
       "4  seg_0042cc  [5, 3, 1, 4, 6, 6, 7, 4, 5, 4, 3, 4, 6, 7, 3, ...    -999"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_pickle('./data/test_features.pkl')\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we split the training data further into a 80/20 training/validation split. We then create dataloaders that will help load the data into the model in parallel using multiprocessing workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data is 0.8001907486886027 of the data\n"
     ]
    }
   ],
   "source": [
    "msk = np.random.rand(len(trainval_df)) < 0.8\n",
    "train_df = trainval_df[msk]\n",
    "valid_df = trainval_df[~msk]\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "valid_df = valid_df.reset_index(drop=True)\n",
    "\n",
    "print('Training data is {} of the data'.format(len(train_df)/len(trainval_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(np.concatenate(train_df.segment.values).astype(float).reshape(-1, 1))\n",
    "train_df.segment = train_df.segment.map(lambda x: scaler.transform(x.astype(float).reshape(-1, 1)).flatten())\n",
    "valid_df.segment = valid_df.segment.map(lambda x: scaler.transform(x.astype(float).reshape(-1, 1)).flatten())\n",
    "test_df.segment = test_df.segment.map(lambda x: scaler.transform(x.astype(float).reshape(-1, 1)).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seg_id</th>\n",
       "      <th>segment</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>[0.6853687744937642, 0.13560972945236838, 0.31...</td>\n",
       "      <td>1.430797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_2</td>\n",
       "      <td>[0.04398322194546909, 0.04398322194546909, 0.3...</td>\n",
       "      <td>1.353196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_3</td>\n",
       "      <td>[0.04398322194546909, -0.8722818531235239, -0....</td>\n",
       "      <td>1.313798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_4</td>\n",
       "      <td>[0.6853687744937642, 0.13560972945236838, -0.0...</td>\n",
       "      <td>1.274400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_5</td>\n",
       "      <td>[0.8686217895075627, 0.22723623695926767, 0.31...</td>\n",
       "      <td>1.236097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    seg_id                                            segment    target\n",
       "0  train_0  [0.6853687744937642, 0.13560972945236838, 0.31...  1.430797\n",
       "1  train_2  [0.04398322194546909, 0.04398322194546909, 0.3...  1.353196\n",
       "2  train_3  [0.04398322194546909, -0.8722818531235239, -0....  1.313798\n",
       "3  train_4  [0.6853687744937642, 0.13560972945236838, -0.0...  1.274400\n",
       "4  train_5  [0.8686217895075627, 0.22723623695926767, 0.31...  1.236097"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bn_function_factory(norm, relu, conv):\n",
    "    def bn_function(*inputs):\n",
    "        concated_features = torch.cat(inputs, 1)\n",
    "        bottleneck_output = conv(relu(norm(concated_features)))\n",
    "        return bottleneck_output\n",
    "\n",
    "    return bn_function\n",
    "\n",
    "\n",
    "class _DenseLayer(nn.Module):\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, efficient=False):\n",
    "        super(_DenseLayer, self).__init__()\n",
    "        self.add_module('norm1', nn.BatchNorm1d(num_input_features)),\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv1', nn.Conv1d(num_input_features, bn_size * growth_rate,\n",
    "                        kernel_size=1, stride=1, bias=False)),\n",
    "        self.add_module('norm2', nn.BatchNorm1d(bn_size * growth_rate)),\n",
    "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv2', nn.Conv1d(bn_size * growth_rate, growth_rate,\n",
    "                        kernel_size=3, stride=1, padding=1, bias=False)),\n",
    "        self.drop_rate = drop_rate\n",
    "        self.efficient = efficient\n",
    "\n",
    "    def forward(self, *prev_features):\n",
    "        bn_function = _bn_function_factory(self.norm1, self.relu1, self.conv1)\n",
    "        if self.efficient and any(prev_feature.requires_grad for prev_feature in prev_features):\n",
    "            bottleneck_output = cp.checkpoint(bn_function, *prev_features)\n",
    "        else:\n",
    "            bottleneck_output = bn_function(*prev_features)\n",
    "        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
    "        return new_features\n",
    "\n",
    "\n",
    "class _Transition(nn.Sequential):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(_Transition, self).__init__()\n",
    "        self.add_module('norm', nn.BatchNorm1d(num_input_features))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "        self.add_module('conv', nn.Conv1d(num_input_features, num_output_features,\n",
    "                                          kernel_size=1, stride=1, bias=False))\n",
    "        self.add_module('pool', nn.AvgPool1d(kernel_size=2, stride=2))\n",
    "\n",
    "\n",
    "class _DenseBlock(nn.Module):\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, efficient=False):\n",
    "        super(_DenseBlock, self).__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(\n",
    "                num_input_features + i * growth_rate,\n",
    "                growth_rate=growth_rate,\n",
    "                bn_size=bn_size,\n",
    "                drop_rate=drop_rate,\n",
    "                efficient=efficient,\n",
    "            )\n",
    "            self.add_module('denselayer%d' % (i + 1), layer)\n",
    "\n",
    "    def forward(self, init_features):\n",
    "        features = [init_features]\n",
    "        for name, layer in self.named_children():\n",
    "            new_features = layer(*features)\n",
    "            features.append(new_features)\n",
    "        return torch.cat(features, 1)\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    r\"\"\"Densenet-BC model class, based on\n",
    "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`\n",
    "    Args:\n",
    "        growth_rate (int) - how many filters to add each layer (`k` in paper)\n",
    "        block_config (list of 3 or 4 ints) - how many layers in each pooling block\n",
    "        num_init_features (int) - the number of filters to learn in the first convolution layer\n",
    "        bn_size (int) - multiplicative factor for number of bottle neck layers\n",
    "            (i.e. bn_size * k features in the bottleneck layer)\n",
    "        drop_rate (float) - dropout rate after each dense layer\n",
    "        num_classes (int) - number of classification classes\n",
    "        small_inputs (bool) - set to True if images are 32x32. Otherwise assumes images are larger.\n",
    "        efficient (bool) - set to True to use checkpointing. Much more memory efficient, but slower.\n",
    "    \"\"\"\n",
    "    def __init__(self, growth_rate=48, block_config=(3, 6, 12, 8), compression=0.5,\n",
    "                 num_init_features=24, bn_size=4, drop_rate=0.2,\n",
    "                 num_classes=1, small_inputs=False, efficient=True):\n",
    "\n",
    "        super(DenseNet, self).__init__()\n",
    "        assert 0 < compression <= 1, 'compression of densenet should be between 0 and 1'\n",
    "        self.avgpool_size = 8 if small_inputs else 7\n",
    "\n",
    "        # First convolution\n",
    "        if small_inputs:\n",
    "            self.features = nn.Sequential(OrderedDict([\n",
    "                ('conv0', nn.Conv1d(1, num_init_features, kernel_size=3, stride=1, padding=1, bias=False)),\n",
    "            ]))\n",
    "        else:\n",
    "            self.features = nn.Sequential(OrderedDict([\n",
    "                ('conv01', nn.Conv1d(1, num_init_features, kernel_size=11, stride=5, padding=5, bias=False)),\n",
    "                ('norm01', nn.BatchNorm1d(num_init_features)),\n",
    "                ('relu01', nn.ReLU(inplace=True)),\n",
    "                ('conv02', nn.Conv1d(num_init_features, num_init_features, kernel_size=11, stride=5, padding=5, bias=False)),\n",
    "                ('norm02', nn.BatchNorm1d(num_init_features)),\n",
    "                ('relu02', nn.ReLU(inplace=True)),\n",
    "                ('conv03', nn.Conv1d(num_init_features, num_init_features, kernel_size=11, stride=5, padding=5, bias=False)),\n",
    "            ]))\n",
    "            self.features.add_module('norm0', nn.BatchNorm1d(num_init_features))\n",
    "            self.features.add_module('relu0', nn.ReLU(inplace=True))\n",
    "            self.features.add_module('pool0', nn.MaxPool1d(kernel_size=3, stride=2, padding=1,\n",
    "                                                           ceil_mode=False))\n",
    "\n",
    "        # Each denseblock\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = _DenseBlock(\n",
    "                num_layers=num_layers,\n",
    "                num_input_features=num_features,\n",
    "                bn_size=bn_size,\n",
    "                growth_rate=growth_rate,\n",
    "                drop_rate=drop_rate,\n",
    "                efficient=efficient,\n",
    "            )\n",
    "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = _Transition(num_input_features=num_features,\n",
    "                                    num_output_features=int(num_features * compression))\n",
    "                self.features.add_module('transition%d' % (i + 1), trans)\n",
    "                num_features = int(num_features * compression)\n",
    "\n",
    "        # Final batch norm\n",
    "        self.features.add_module('norm_final', nn.BatchNorm1d(num_features))\n",
    "\n",
    "        # Linear layer\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "\n",
    "        # Initialization\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'conv' in name and 'weight' in name:\n",
    "                n = param.size(0) * param.size(2)\n",
    "                param.data.normal_().mul_(math.sqrt(2. / n))\n",
    "            elif 'norm' in name and 'weight' in name:\n",
    "                param.data.fill_(1)\n",
    "            elif 'norm' in name and 'bias' in name:\n",
    "                param.data.fill_(0)\n",
    "            elif 'classifier' in name and 'bias' in name:\n",
    "                param.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.adaptive_avg_pool1d(out, 1).view(features.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1            [-1, 24, 30000]             264\n",
      "       BatchNorm1d-2            [-1, 24, 30000]              48\n",
      "              ReLU-3            [-1, 24, 30000]               0\n",
      "            Conv1d-4             [-1, 24, 6000]           6,336\n",
      "       BatchNorm1d-5             [-1, 24, 6000]              48\n",
      "              ReLU-6             [-1, 24, 6000]               0\n",
      "            Conv1d-7             [-1, 24, 1200]           6,336\n",
      "       BatchNorm1d-8             [-1, 24, 1200]              48\n",
      "              ReLU-9             [-1, 24, 1200]               0\n",
      "        MaxPool1d-10              [-1, 24, 600]               0\n",
      "      BatchNorm1d-11              [-1, 24, 600]              48\n",
      "             ReLU-12              [-1, 24, 600]               0\n",
      "           Conv1d-13             [-1, 192, 600]           4,608\n",
      "      BatchNorm1d-14             [-1, 192, 600]             384\n",
      "             ReLU-15             [-1, 192, 600]               0\n",
      "           Conv1d-16              [-1, 48, 600]          27,648\n",
      "      _DenseLayer-17              [-1, 48, 600]               0\n",
      "      BatchNorm1d-18              [-1, 72, 600]             144\n",
      "             ReLU-19              [-1, 72, 600]               0\n",
      "           Conv1d-20             [-1, 192, 600]          13,824\n",
      "      BatchNorm1d-21             [-1, 192, 600]             384\n",
      "             ReLU-22             [-1, 192, 600]               0\n",
      "           Conv1d-23              [-1, 48, 600]          27,648\n",
      "      _DenseLayer-24              [-1, 48, 600]               0\n",
      "      BatchNorm1d-25             [-1, 120, 600]             240\n",
      "             ReLU-26             [-1, 120, 600]               0\n",
      "           Conv1d-27             [-1, 192, 600]          23,040\n",
      "      BatchNorm1d-28             [-1, 192, 600]             384\n",
      "             ReLU-29             [-1, 192, 600]               0\n",
      "           Conv1d-30              [-1, 48, 600]          27,648\n",
      "      _DenseLayer-31              [-1, 48, 600]               0\n",
      "      _DenseBlock-32             [-1, 168, 600]               0\n",
      "      BatchNorm1d-33             [-1, 168, 600]             336\n",
      "             ReLU-34             [-1, 168, 600]               0\n",
      "           Conv1d-35              [-1, 84, 600]          14,112\n",
      "        AvgPool1d-36              [-1, 84, 300]               0\n",
      "      BatchNorm1d-37              [-1, 84, 300]             168\n",
      "             ReLU-38              [-1, 84, 300]               0\n",
      "           Conv1d-39             [-1, 192, 300]          16,128\n",
      "      BatchNorm1d-40             [-1, 192, 300]             384\n",
      "             ReLU-41             [-1, 192, 300]               0\n",
      "           Conv1d-42              [-1, 48, 300]          27,648\n",
      "      _DenseLayer-43              [-1, 48, 300]               0\n",
      "      BatchNorm1d-44             [-1, 132, 300]             264\n",
      "             ReLU-45             [-1, 132, 300]               0\n",
      "           Conv1d-46             [-1, 192, 300]          25,344\n",
      "      BatchNorm1d-47             [-1, 192, 300]             384\n",
      "             ReLU-48             [-1, 192, 300]               0\n",
      "           Conv1d-49              [-1, 48, 300]          27,648\n",
      "      _DenseLayer-50              [-1, 48, 300]               0\n",
      "      BatchNorm1d-51             [-1, 180, 300]             360\n",
      "             ReLU-52             [-1, 180, 300]               0\n",
      "           Conv1d-53             [-1, 192, 300]          34,560\n",
      "      BatchNorm1d-54             [-1, 192, 300]             384\n",
      "             ReLU-55             [-1, 192, 300]               0\n",
      "           Conv1d-56              [-1, 48, 300]          27,648\n",
      "      _DenseLayer-57              [-1, 48, 300]               0\n",
      "      BatchNorm1d-58             [-1, 228, 300]             456\n",
      "             ReLU-59             [-1, 228, 300]               0\n",
      "           Conv1d-60             [-1, 192, 300]          43,776\n",
      "      BatchNorm1d-61             [-1, 192, 300]             384\n",
      "             ReLU-62             [-1, 192, 300]               0\n",
      "           Conv1d-63              [-1, 48, 300]          27,648\n",
      "      _DenseLayer-64              [-1, 48, 300]               0\n",
      "      BatchNorm1d-65             [-1, 276, 300]             552\n",
      "             ReLU-66             [-1, 276, 300]               0\n",
      "           Conv1d-67             [-1, 192, 300]          52,992\n",
      "      BatchNorm1d-68             [-1, 192, 300]             384\n",
      "             ReLU-69             [-1, 192, 300]               0\n",
      "           Conv1d-70              [-1, 48, 300]          27,648\n",
      "      _DenseLayer-71              [-1, 48, 300]               0\n",
      "      BatchNorm1d-72             [-1, 324, 300]             648\n",
      "             ReLU-73             [-1, 324, 300]               0\n",
      "           Conv1d-74             [-1, 192, 300]          62,208\n",
      "      BatchNorm1d-75             [-1, 192, 300]             384\n",
      "             ReLU-76             [-1, 192, 300]               0\n",
      "           Conv1d-77              [-1, 48, 300]          27,648\n",
      "      _DenseLayer-78              [-1, 48, 300]               0\n",
      "      _DenseBlock-79             [-1, 372, 300]               0\n",
      "      BatchNorm1d-80             [-1, 372, 300]             744\n",
      "             ReLU-81             [-1, 372, 300]               0\n",
      "           Conv1d-82             [-1, 186, 300]          69,192\n",
      "        AvgPool1d-83             [-1, 186, 150]               0\n",
      "      BatchNorm1d-84             [-1, 186, 150]             372\n",
      "             ReLU-85             [-1, 186, 150]               0\n",
      "           Conv1d-86             [-1, 192, 150]          35,712\n",
      "      BatchNorm1d-87             [-1, 192, 150]             384\n",
      "             ReLU-88             [-1, 192, 150]               0\n",
      "           Conv1d-89              [-1, 48, 150]          27,648\n",
      "      _DenseLayer-90              [-1, 48, 150]               0\n",
      "      BatchNorm1d-91             [-1, 234, 150]             468\n",
      "             ReLU-92             [-1, 234, 150]               0\n",
      "           Conv1d-93             [-1, 192, 150]          44,928\n",
      "      BatchNorm1d-94             [-1, 192, 150]             384\n",
      "             ReLU-95             [-1, 192, 150]               0\n",
      "           Conv1d-96              [-1, 48, 150]          27,648\n",
      "      _DenseLayer-97              [-1, 48, 150]               0\n",
      "      BatchNorm1d-98             [-1, 282, 150]             564\n",
      "             ReLU-99             [-1, 282, 150]               0\n",
      "          Conv1d-100             [-1, 192, 150]          54,144\n",
      "     BatchNorm1d-101             [-1, 192, 150]             384\n",
      "            ReLU-102             [-1, 192, 150]               0\n",
      "          Conv1d-103              [-1, 48, 150]          27,648\n",
      "     _DenseLayer-104              [-1, 48, 150]               0\n",
      "     BatchNorm1d-105             [-1, 330, 150]             660\n",
      "            ReLU-106             [-1, 330, 150]               0\n",
      "          Conv1d-107             [-1, 192, 150]          63,360\n",
      "     BatchNorm1d-108             [-1, 192, 150]             384\n",
      "            ReLU-109             [-1, 192, 150]               0\n",
      "          Conv1d-110              [-1, 48, 150]          27,648\n",
      "     _DenseLayer-111              [-1, 48, 150]               0\n",
      "     BatchNorm1d-112             [-1, 378, 150]             756\n",
      "            ReLU-113             [-1, 378, 150]               0\n",
      "          Conv1d-114             [-1, 192, 150]          72,576\n",
      "     BatchNorm1d-115             [-1, 192, 150]             384\n",
      "            ReLU-116             [-1, 192, 150]               0\n",
      "          Conv1d-117              [-1, 48, 150]          27,648\n",
      "     _DenseLayer-118              [-1, 48, 150]               0\n",
      "     BatchNorm1d-119             [-1, 426, 150]             852\n",
      "            ReLU-120             [-1, 426, 150]               0\n",
      "          Conv1d-121             [-1, 192, 150]          81,792\n",
      "     BatchNorm1d-122             [-1, 192, 150]             384\n",
      "            ReLU-123             [-1, 192, 150]               0\n",
      "          Conv1d-124              [-1, 48, 150]          27,648\n",
      "     _DenseLayer-125              [-1, 48, 150]               0\n",
      "     BatchNorm1d-126             [-1, 474, 150]             948\n",
      "            ReLU-127             [-1, 474, 150]               0\n",
      "          Conv1d-128             [-1, 192, 150]          91,008\n",
      "     BatchNorm1d-129             [-1, 192, 150]             384\n",
      "            ReLU-130             [-1, 192, 150]               0\n",
      "          Conv1d-131              [-1, 48, 150]          27,648\n",
      "     _DenseLayer-132              [-1, 48, 150]               0\n",
      "     BatchNorm1d-133             [-1, 522, 150]           1,044\n",
      "            ReLU-134             [-1, 522, 150]               0\n",
      "          Conv1d-135             [-1, 192, 150]         100,224\n",
      "     BatchNorm1d-136             [-1, 192, 150]             384\n",
      "            ReLU-137             [-1, 192, 150]               0\n",
      "          Conv1d-138              [-1, 48, 150]          27,648\n",
      "     _DenseLayer-139              [-1, 48, 150]               0\n",
      "     BatchNorm1d-140             [-1, 570, 150]           1,140\n",
      "            ReLU-141             [-1, 570, 150]               0\n",
      "          Conv1d-142             [-1, 192, 150]         109,440\n",
      "     BatchNorm1d-143             [-1, 192, 150]             384\n",
      "            ReLU-144             [-1, 192, 150]               0\n",
      "          Conv1d-145              [-1, 48, 150]          27,648\n",
      "     _DenseLayer-146              [-1, 48, 150]               0\n",
      "     BatchNorm1d-147             [-1, 618, 150]           1,236\n",
      "            ReLU-148             [-1, 618, 150]               0\n",
      "          Conv1d-149             [-1, 192, 150]         118,656\n",
      "     BatchNorm1d-150             [-1, 192, 150]             384\n",
      "            ReLU-151             [-1, 192, 150]               0\n",
      "          Conv1d-152              [-1, 48, 150]          27,648\n",
      "     _DenseLayer-153              [-1, 48, 150]               0\n",
      "     BatchNorm1d-154             [-1, 666, 150]           1,332\n",
      "            ReLU-155             [-1, 666, 150]               0\n",
      "          Conv1d-156             [-1, 192, 150]         127,872\n",
      "     BatchNorm1d-157             [-1, 192, 150]             384\n",
      "            ReLU-158             [-1, 192, 150]               0\n",
      "          Conv1d-159              [-1, 48, 150]          27,648\n",
      "     _DenseLayer-160              [-1, 48, 150]               0\n",
      "     BatchNorm1d-161             [-1, 714, 150]           1,428\n",
      "            ReLU-162             [-1, 714, 150]               0\n",
      "          Conv1d-163             [-1, 192, 150]         137,088\n",
      "     BatchNorm1d-164             [-1, 192, 150]             384\n",
      "            ReLU-165             [-1, 192, 150]               0\n",
      "          Conv1d-166              [-1, 48, 150]          27,648\n",
      "     _DenseLayer-167              [-1, 48, 150]               0\n",
      "     _DenseBlock-168             [-1, 762, 150]               0\n",
      "     BatchNorm1d-169             [-1, 762, 150]           1,524\n",
      "            ReLU-170             [-1, 762, 150]               0\n",
      "          Conv1d-171             [-1, 381, 150]         290,322\n",
      "       AvgPool1d-172              [-1, 381, 75]               0\n",
      "     BatchNorm1d-173              [-1, 381, 75]             762\n",
      "            ReLU-174              [-1, 381, 75]               0\n",
      "          Conv1d-175              [-1, 192, 75]          73,152\n",
      "     BatchNorm1d-176              [-1, 192, 75]             384\n",
      "            ReLU-177              [-1, 192, 75]               0\n",
      "          Conv1d-178               [-1, 48, 75]          27,648\n",
      "     _DenseLayer-179               [-1, 48, 75]               0\n",
      "     BatchNorm1d-180              [-1, 429, 75]             858\n",
      "            ReLU-181              [-1, 429, 75]               0\n",
      "          Conv1d-182              [-1, 192, 75]          82,368\n",
      "     BatchNorm1d-183              [-1, 192, 75]             384\n",
      "            ReLU-184              [-1, 192, 75]               0\n",
      "          Conv1d-185               [-1, 48, 75]          27,648\n",
      "     _DenseLayer-186               [-1, 48, 75]               0\n",
      "     BatchNorm1d-187              [-1, 477, 75]             954\n",
      "            ReLU-188              [-1, 477, 75]               0\n",
      "          Conv1d-189              [-1, 192, 75]          91,584\n",
      "     BatchNorm1d-190              [-1, 192, 75]             384\n",
      "            ReLU-191              [-1, 192, 75]               0\n",
      "          Conv1d-192               [-1, 48, 75]          27,648\n",
      "     _DenseLayer-193               [-1, 48, 75]               0\n",
      "     BatchNorm1d-194              [-1, 525, 75]           1,050\n",
      "            ReLU-195              [-1, 525, 75]               0\n",
      "          Conv1d-196              [-1, 192, 75]         100,800\n",
      "     BatchNorm1d-197              [-1, 192, 75]             384\n",
      "            ReLU-198              [-1, 192, 75]               0\n",
      "          Conv1d-199               [-1, 48, 75]          27,648\n",
      "     _DenseLayer-200               [-1, 48, 75]               0\n",
      "     BatchNorm1d-201              [-1, 573, 75]           1,146\n",
      "            ReLU-202              [-1, 573, 75]               0\n",
      "          Conv1d-203              [-1, 192, 75]         110,016\n",
      "     BatchNorm1d-204              [-1, 192, 75]             384\n",
      "            ReLU-205              [-1, 192, 75]               0\n",
      "          Conv1d-206               [-1, 48, 75]          27,648\n",
      "     _DenseLayer-207               [-1, 48, 75]               0\n",
      "     BatchNorm1d-208              [-1, 621, 75]           1,242\n",
      "            ReLU-209              [-1, 621, 75]               0\n",
      "          Conv1d-210              [-1, 192, 75]         119,232\n",
      "     BatchNorm1d-211              [-1, 192, 75]             384\n",
      "            ReLU-212              [-1, 192, 75]               0\n",
      "          Conv1d-213               [-1, 48, 75]          27,648\n",
      "     _DenseLayer-214               [-1, 48, 75]               0\n",
      "     BatchNorm1d-215              [-1, 669, 75]           1,338\n",
      "            ReLU-216              [-1, 669, 75]               0\n",
      "          Conv1d-217              [-1, 192, 75]         128,448\n",
      "     BatchNorm1d-218              [-1, 192, 75]             384\n",
      "            ReLU-219              [-1, 192, 75]               0\n",
      "          Conv1d-220               [-1, 48, 75]          27,648\n",
      "     _DenseLayer-221               [-1, 48, 75]               0\n",
      "     BatchNorm1d-222              [-1, 717, 75]           1,434\n",
      "            ReLU-223              [-1, 717, 75]               0\n",
      "          Conv1d-224              [-1, 192, 75]         137,664\n",
      "     BatchNorm1d-225              [-1, 192, 75]             384\n",
      "            ReLU-226              [-1, 192, 75]               0\n",
      "          Conv1d-227               [-1, 48, 75]          27,648\n",
      "     _DenseLayer-228               [-1, 48, 75]               0\n",
      "     _DenseBlock-229              [-1, 765, 75]               0\n",
      "     BatchNorm1d-230              [-1, 765, 75]           1,530\n",
      "          Linear-231                    [-1, 1]             766\n",
      "================================================================\n",
      "Total params: 3,383,542\n",
      "Trainable params: 3,383,542\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 86.50\n",
      "Params size (MB): 12.91\n",
      "Estimated Total Size (MB): 99.97\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = DenseNet().to(device)\n",
    "summary(model, (1, 150000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LANL_Ressubblock(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size,\n",
    "                 padding):\n",
    "        super(LANL_Ressubblock, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, padding=padding)\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class LANL_Resblock(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels):\n",
    "        super(LANL_Resblock, self).__init__()\n",
    "        \n",
    "        self.subblocks = nn.Sequential(\n",
    "            LANL_Ressubblock(in_channels, out_channels, 7, 3),\n",
    "            nn.ReLU(),\n",
    "            LANL_Ressubblock(out_channels, out_channels, 5, 2),\n",
    "            nn.ReLU(),\n",
    "            LANL_Ressubblock(out_channels, out_channels, 3, 1)\n",
    "        )\n",
    "        \n",
    "        if (in_channels != out_channels):\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, 1),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.shortcut = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.subblocks(x)\n",
    "        x2 = self.shortcut(x)\n",
    "        x = x1 + x2\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class LANL_Resnext(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 n_output=1):\n",
    "        \n",
    "        super(LANL_Resnext, self).__init__()\n",
    "        self.blocks = nn.Sequential(\n",
    "            LANL_Resblock(1, 64),\n",
    "            LANL_Resblock(64, 128),\n",
    "            LANL_Resblock(128, 128)\n",
    "        )\n",
    "        \n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(128, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.blocks(x)\n",
    "        x = self.gap(x).squeeze(-1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate search finished. See the graph with {finder_name}.plot()\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEOCAYAAABmVAtTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xd8XnX5//HXlT2a0ZF0pntAN21oS9l7iAXZIFOkgiAgKop+9aeoX8dXZAuUJYgMZZYNRcruhLZ0QPde6Uqzm3H9/sjdGELaJm3u+9x38n4+HufBWfc5b+PdXDnnc87nY+6OiIgIQFzQAUREJHqoKIiISB0VBRERqaOiICIidVQURESkjoqCiIjUUVEQEZE6KgoiIlJHRUFEROqoKIiISJ2EoAM0V6dOnbx3795BxxARiSmzZ8/e4u45+9ov5opC7969mTVrVtAxRERiipmtasp+un0kIiJ1VBRERKSOioKIiNQJa1Ews2wze9bMvjCzRWZ2WIPtZmZ3mdlSM5tnZqPCmUdERPYu3A3NdwJvuPs5ZpYEpDXYfiowIDSNBe4L/VdERAIQtisFM8sEjgIeBnD3Xe6+o8FuZwCPe61pQLaZdQ1XJhER2btw3j7qCxQAj5rZZ2b2kJmlN9inO7Cm3vLa0LqwcncWbypi6ebicJ9KRCSmhPP2UQIwCviBu083szuBnwG/rLePNfK5rw0abWYTgYkAPXv23K8wNTXOZ2u28+aCTby5YCOrtpYCML5fR644vA/HHZRLfFxjcURE2o5wFoW1wFp3nx5afpbaotBwn7x6yz2A9Q0P5O6TgEkA+fn5XysaTfHs7LXc/Nw8EuONw/p1YuJRfSkqr+Lxj1dy1eOz6NUxjSuP6MO5o/NITYqvf26WFRQTZ0afTumYqXCISOsVtqLg7hvNbI2ZDXL3L4HjgYUNdpsMXGdmT1PbwFzo7hvCkee4g3O584KRHDMol6zUxLr13z2iD28u2MTDHy7nVy8t4Pa3F3PpYb05elAOU7/YzKufb2BZQQkAORnJjO3TgTF9OtC7YzrdslPplp1CWlLMvRguItIoc9+vP7ybdnCzkcBDQBKwHLgCOB/A3e+32j+77wFOAUqBK9x9r31Y5Ofne7i6uZi1chv3v7ecKYs2ARBnMLZPR04b3pV4M6av2Mr05dvYuLP8K5/LzUjmkJ7ZjOrZntG92jO8RzZJCXoFRESih5nNdvf8fe4XzqIQDuEsCrst3VzE5+sKOaJ/DjkZyV/Z5u5sKCxn7fYyNhSWsW5HGUs2FfPp6u117RQZKQkcd1AuJw/pwtEDc0hP1pWEiASrqUVBv60a0T83g/65GY1uM7PQbaPUr20rKKpg9qptTFm0mXcWbeKlOetJSojj6IE5nDasC8cf3JnMlMRGjioiEh1UFFpQTkYypwztyilDu1JVXcOsVdt5Y/5G3pi/kbcXbiIx3hjbpyNj+3RgbN+ODO+RRXWNs25HGeu2l1FeWc3Q7ln0aJ+qBm0RCYRuH0VA7eOwO3j98w18uHQLX2wsAiA+zqiu+frPPycjmVE9sxnWPYtBXTI5qEsGXbJS+HxdIZ8s28rHy7awpWgXfTql0z+3Hf1y0zl2UC7ZaUmR/p8mIjFCt4+iSFycMbpXbSM0wPaSXcxcuY05a3bQLiWB7tmp9GifSkJcHPPW7uDT1Tv4dHXtOxW7mcHu+n1w10zyOqSyeHMRby/aRHWN0z4tkZ+echDn5ecRp/ctRGQ/6UohihVXVLF4UxFfbixizbZShnTLYlzfDnRs99/G711VNcxfX8gfX/uCGSu3MTIvm9+eMZRhPbL2efzNReWs2VbGgM7t1NYh0srp6aM2xt154bN1/O9ri9hSvItBnTM47uBcjj8ol7457Sgsq6SwrJItRRXMWLmN9xcX1N3GAujRPpXBXTPplp1KenI87ZITSUuKp7rGqaqpobLa6Z6dymnDuupxW5EYpKLQRhWWVfLvWWt4Z9FmZq7cRlUjbRZJ8XHk927PkQNy6JeTzpLNxSzasJOFG3ZSUFRBSUUVjXwMgO7ZqVx9TD/OHd2DlMT4xncSkaijoiAUllXywZICNu+sIDstkazURLLTEhncNesrXXk05O6UV9ZQuquK+DgjIT6OhDjjk+VbufudJXy6egedM5O5cExPzhzZnd6dGvZzKCLRRkVBwsLd+WTZVv42dRkfLduCO4zqmc0pQ7uQlZpIQlwcCfFG+7QkenVMo1t2Konxut0kEjQ9fSRhYWaM79+J8f07sX5HGZPnrueFT9fxv6990ej+8XFGl8wU4uJqG8V3VdVQ45CaGE9aUjypSfGMyMvmojE9Gdp9343jIhJeulKQFrG1uIKKqhoqq2unrcW7WLWtlNVbS1m3owyobctISojDDMp2VVNaWU1ReRXTl2+loqqG4T2yODc/j8FdM+ialUpuRjIJusoQaRG6UpCIqv+YLED/XBjbt2OTPltYWsmLc9bx5PTV/PLF+XXr4+OM3IxkOmem0DkzmS6ZKZw9ugfDe2S3aHYR+S9dKUjUcHeWbylhzbZSNhSWs35HGRsKy9m0s3Zau72Miqoarjm6H9cfP0CPxoo0g64UJOaYGf1y2tEvp12j2wvLKvntKwu5592lTFm0idvOG8GQbmqHEGlJ+lNLYkZWaiJ/OXcED12az5biXZx570e8+Nm6oGOJtCoqChJzThjcmbd/eBSjerbnxmfmcO+7S4m126Ai0UpFQWJS+/QkHr9yDBNGdOP/3vySX7w4n6rqmqBjicQ8tSlIzEpOiOeO80fSvX0q901dxowV2zhmYA7j+3dkTJ+OtNOIdyLNpn81EtPi4oyfnnIQgzpn8MzMNTw+bRUPfbiChDhjwshufP+Y/vTPbbzhWkS+To+kSqtSXlnN7FXbeXvhJp6euZqKqhpOG9qV7x7ZhxE9sjXWhLRZ6vtI2rytxRU88tEKHvt4FcUVVWSmJHBo7w6M6dOB00d0o3sj42yLtFYqCiIhhWWVTFm4iZkrtzFjxTaWbykhJTGOG44fyHeP7KMO+6RNiIqiYGYrgSKgGqhqGMjMjgFeAlaEVj3v7rfu7ZgqCnKg1mwr5bevLOSthZsY2Lkdv//WMA7t3SHoWCJh1dSiEIk/kY5195F7CfNBaPvIfRUEkZaQ1yGNSZfm8+Cl+ZRUVHPu/Z/ww2fmsLGwPOhoIoHTdbO0WScO7szbNx3Ftcf249XPN3DcbVO5992llFdWBx1NJDDhLgoOvGVms81s4h72OczM5prZ62Y2JMx5RL4iLSmBn5x8EFN+eDRHDujE/735Jafd9QFLNhXt+8MirVC42xS6uft6M8sF3gZ+4O7v19ueCdS4e7GZnQbc6e4DGjnORGAiQM+ePUevWrUqbJmlbXt/cQE3/WsOZbuque28kZwytEvQkURaRFS0Kbj7+tB/NwMvAGMabN/p7sWh+deARDPr1MhxJrl7vrvn5+TkhDOytHFHDczh5R8cQf/OGVz9xGz+8uaXVNfE1hN6IgcibEXBzNLNLGP3PHASML/BPl3MzELzY0J5toYrk0hTdM1K5ZmJ4zgvvwf3vLuUo/78Lre99SUrt5QEHU0k7MLZzUVn4IXQ7/wE4El3f8PMrgZw9/uBc4BrzKwKKAMu8Fh7cUJapZTEeP509nCOO6gzT85YzT3vLuXu/yxlfL+O3HvRKNqnJwUdUSQs9PKaSBNsKCzj+U/Xcec7SxjaLZMnrxpHSmJ80LFEmiwq2hREWouuWalce2x/7jx/JJ+t2cENT3+mtgZplVQURJrh1GFd+eU3BvPmgk3c+vICDe4jrY66zhZppu8c0Yf1O8p46MMVdEhP5vrj+xNqOxOJeSoKIvvh56cdzNaSXdw+ZTFLC4r509nDSEvSPyeJffoWi+yHuDjjr+eNYEDndvzfm1+yZFMRD1wyml4d04OOJnJA1KYgsp/MjO8f05+/XzGGDYXlfPPuD/lyo7rHkNimoiBygI4emMPL1x2BA3dMWRx0HJEDoqIg0gJ6dkzjknG9eGPBRlbozWeJYSoKIi3kisNrR3Gb9P7yoKOI7DcVBZEWkpORzDmje/Dcp2vZXKQBeyQ2qSiItKCJR/alqrqGRz9aGXQUkf2ioiDSgnp3SufUoV15Ytoqisorg44j0mwqCiIt7HtH96WovIqnZqwOOopIs6koiLSw4T2yGd+vIw99sILiiqqg44g0i4qCSBj8+ORBFBRXcNtbXwYdRaRZVBREwmBUz/ZcMq4Xf/94JXPW7Ag6jkiTqSiIhMlPTh5E54wUbnn+cyqra4KOI9IkKgoiYZKRkshvzhjCog07efjDFUHHEWkSFQWRMDp5SBdOHtKZO6YsZtVWdX8h0U9FQSTMfjNhKAlxcfzPi/M1UptEPRUFkTDrkpXCzacM4oMlW5g8d33QcUT2SkVBJAK+PbYXI/Ky+e0rCyks1ZvOEr1UFEQiID7O+N9vDWV7aSV/fOOLoOOI7FFYi4KZrTSzz81sjpnNamS7mdldZrbUzOaZ2ahw5hEJ0pBuWXzn8N48NWM1s1ZuCzqOSKMicaVwrLuPdPf8RradCgwITROB+yKQRyQwN54wkO7Zqfz8hc/ZVaV3FyT6BH376Azgca81Dcg2s64BZxIJm/TkBP7fNwezeFMxL362Lug4Il8T7qLgwFtmNtvMJjayvTuwpt7y2tA6kVbrxMGdGdItk/vfX0ZNjR5RlegS7qJwuLuPovY20bVmdlSD7dbIZ772r8TMJprZLDObVVBQEI6cIhFjZnzv6H4sLyjhrYWbgo4j8hVhLQruvj70383AC8CYBrusBfLqLfcAvvYgt7tPcvd8d8/PyckJV1yRiDltaBfyOqRy/3vL9EKbRJWwFQUzSzezjN3zwEnA/Aa7TQYuDT2FNA4odPcN4cokEi0S4uOYeGRf5qzZwfQVehJJokc4rxQ6Ax+a2VxgBvCqu79hZleb2dWhfV4DlgNLgQeB74cxj0hUOTc/j47pSdz/3rKgo4jUSQjXgd19OTCikfX315t34NpwZRCJZimJ8Vw+vje3vb2YRRt2cnDXzKAjiQT+SKpIm3bJYb1IS4rnAV0tSJRQURAJUHZaEucfmscr8zaweWd50HFEVBREgnbpYb2pqnGenLE66CgiKgoiQevTKZ2jB+bw5PTVGrZTAqeiIBIFLhvfi81FFby5YGPQUaSNU1EQiQJHD8wlr0Mqj3+yKugo0sapKIhEgfg445JxvZixYhuLNuwMOo60YSoKIlHivPw8khPidLUggVJREIkS2WlJnDmyOy9+to7CMg3ZKcFQURCJIpcc1ouyymqe0uOpEhAVBZEoMrR7FkcNzOGB95ZRVK6rBYk8FQWRKPOTkwaxvbSSBz9YEXQUaYNUFESizLAeWZw2rAsPf7CcrcUVQceRKPHwhyuYEYFu1lUURKLQTScOoqyymnvfVUd5Au7OH15bxHuLN4f9XCoKIlGof247zhndgyemrWLdjrKg40jAKqpqqKpx0pPDNtpBHRUFkSh1wwkDAbhrypKAk0jQiiuqAMhQURBpu7pnp/LtcT359+w1fLFRbzm3ZcXltUWhXYqKgkibdv1xA8hMTeTXkxdQO1ChtEW7rxTSk1QURNq09ulJ/OikQUxbvo1XP98QdBwJyO6ioCsFEeGiMT0Z3DWT37+6iNJdVUHHkQDU3T5Sm4KIxMcZt54xhA2F5dz77tKg40gA6q4UVBREBCC/dwe+dUh3Hnx/BSu3lAQdRyIs6m4fmVk/M0sOzR9jZtebWXYTPxtvZp+Z2SuNbLvczArMbE5o+m7z4ou0HbecehCJ8cZvXlajc1sTjVcKzwHVZtYfeBjoAzzZxM/eACzay/Zn3H1kaHqoiccUaXNyM1P44YkDeffLAl6fr2E725KSiiriDFIT48N+rqYWhRp3rwK+Bdzh7j8Euu7rQ2bWA/gGoF/2Ii3g8vG9GdItk19PXsBO9aLaZhSVV5GenICZhf1cTS0KlWZ2IXAZsPs2UGITPncHcDNQs5d9zjazeWb2rJnlNTGPSJuUEB/HH84axpbiCm5788ug40iEFFdUReRtZmh6UbgCOAz4vbuvMLM+wBN7+4CZnQ5sdvfZe9ntZaC3uw8HpgCP7eFYE81slpnNKigoaGJkkdZpeI9sLj2sN49PW8WcNTuCjiMRUFJRFZFGZmhiUXD3he5+vbs/ZWbtgQx3/+M+PnY4MMHMVgJPA8eZ2VcKibtvdffdfQM/CIzew/knuXu+u+fn5OQ0JbJIq/ajkwaSm5HMz5//nKrqvV2IS2tQXFEVkc7woOlPH001s0wz6wDMBR41s7/u7TPufou793D33sAFwH/c/eIGx63fLjGBvTdIi0hIRkoiv5kwhIUbdnL/e+peu7UrKq+KyJNH0PTbR1nuvhM4C3jU3UcDJ+zPCc3sVjObEFq83swWmNlc4Hrg8v05pkhbdPKQLpw+vCt3TFnCvLW6jdSalVRUkRFNt4+AhNBf9efx34bmJnP3qe5+emj+V+4+OTR/i7sPcfcR7n6su3/R3GOLtFVmxu/PHEZORjI3PjOHsl3VQUeSMCmuqIpIZ3jQ9KJwK/AmsMzdZ5pZX0CdvIsELCstkdvOHcHyghL+9zXdfW2tiqOwofnf7j7c3a8JLS9397PDG01EmmJ8/05cdWQf/jFtFe9+Ef7hGiWy3L22KERTm4KZ9TCzF8xss5ltMrPnQi+miUgU+PHJgzioSwY/eXYehWV6qa01Kd1VjXtkuriApt8+ehSYDHQDulP7fsGj4QolIs2TnBDPX84dwbaSCv76ll5qa01KItgZHjS9KOS4+6PuXhWa/g7ohQGRKDK0exYXj+vFP6atYv66wqDjSAspimBneND0orDFzC4O9Xgab2YXA1vDGUxEmu9HJw6ifVoSv3xpPjU16km1NYjkADvQ9KLwHWofR90IbADOobbrCxGJIllpidxy2sF8tnoHz85eG3QcaQEl0Xil4O6r3X2Cu+e4e667n0nti2wiEmXOHtWdQ3u3549vfMGO0l1Bx5EDtPv2UVR1c7EHN7VYChFpMWbGrWcMpbCskt+/qncXYt3u20fR9kZzY8LfsbeI7JeDu2Zy9dF9+ffstbyhAXliWsmu2LlSUCuWSBS74fiBDO2eyS3Pz2PzzvKg48h+KoqmhmYzKzKznY1MRdS+syAiUSopIY47zh9J6a5qbn5unsZ1jlElFVUkxhvJCQfyN3zT7fUs7p7h7pmNTBnuHpmyJSL7rX9uBj8/7WCmflnAE9NWBR1H9sPusRQiMRQnHNjtIxGJAZce1oujBubw+9cWsXhTUdBxpJmKIziWAqgoiLR6ZsZfzhlOu+RErnliNsWhRxwlNkSyMzxQURBpE3IzU7jrwpGs2FLCz9S+EFNUFEQkLMb368SPThrEK/M28Pgnal+IFZEcSwFUFETalGuO7sfxB+Xyu1cX8unq7UHHkSbY3dAcKSoKIm1IXJzx1/NG0jkzhR88+Rk7yzX2QrQrLq8iQ0VBRMIlKy2Ruy88hI07y/n1SwuCjiP7UKI2BREJt0N6tue6Y/vz/GfreHXehqDjyB5U1zglu6p1+0hEwu+64/ozIi+bn7/wORsL1Q1GNNrd71GkOsMDFQWRNisxPo7bzxvBrqoafvLsXA3KE4VKItxtNkSgKIRGavvMzF5pZFuymT1jZkvNbLqZ9Q53HhH5r7457fjFNw7mgyVbeOyTlUHHkQYiPeoaROZK4QZgT526Xwlsd/f+wO3AnyKQR0Tq+fbYnhx3UC5/fP0LdYMRZerGZ24tt4/MrAfwDeChPexyBvBYaP5Z4HiLVK9PIgLUdoPxp7OH0y45gRufnsOuqpqgI0lIpIfihPBfKdwB3Azs6VvWHVgD4O5VQCHQseFOZjbRzGaZ2ayCgoJwZRVps3Iykvnj2cNZuGEnf317cdBxJKRV3T4ys9OBze4+e2+7NbLua61d7j7J3fPdPT8nJ6fFMorIf504uDMXjsnjgfeXMX351qDjCNR1XtgqigJwODDBzFYCTwPHmdkTDfZZC+QBmFkCkAVsC2MmEdmL//nGYHp1SOOmf82lSG87B65VFQV3v8Xde7h7b+AC4D/ufnGD3SYDl4Xmzwnto+fiRAKSnpzAbeeNZH1hGX9644ug47R5u28ftapHUhsys1vNbEJo8WGgo5ktBW4CfhbpPCLyVaN7tefKw/vwxLTVfLJMt5GCVLyriqSEOJIiNBQnRKgouPtUdz89NP8rd58cmi9393Pdvb+7j3H35ZHIIyJ796OTBtGrYxo/e34eZbuqg47TZkW6MzzQG80i0ojUpHj+eNZwVm0t5ba3vgw6TpsV6bEUQEVBRPbgsH4duXhcTx7+aIXGXghISUUV6UkqCiISJX526sF0y0rlpmfm6GmkABSV60pBRKJIu+QEbj9/JKu3lXLL859rbOcIK9kV2bEUQEVBRPZhTJ8OdWM7/3P66qDjtCnF5SoKIhKFrjm6H0cNzOHWVxYyf11h0HHaDDU0i0hUioszbj9vBO3TErnuyU81tnOEFEd4KE5QURCRJurYLpm7LxzF2u1lXPbIDDU8h1lldQ3llTUqCiISvcb06cA9F43i87WFXPrIDF0xhFEQ3WaDioKINNMpQ7vUFYbLVBjCJojO8EBFQUT2wylDu3Dvt2sLwxWPztTAPGFQHMCoa6CiICL76eQhXbj9/JHMXrWd295WVxgtLYgeUkFFQUQOwDdHdOOisT154L3lfLhkS9BxWhXdPhKRmPTLbwymX046N/1rDttKdgUdp9XYXRQydPtIRGJJalI8d114CDtKK7n52XnqCqOF7H76SLePRCTmDOmWxU9PPYgpizbx6Ecrg47TKhSV6/aRiMSwK8b35sTBnfndqwt5a8HGoOPEvN23j9KT4iN6XhUFEWkRcXHGnReMZFiPbK5/+jONwXCASiqqSE2MJyE+sr+mVRREpMWkJSXw8GX5dM5M4buPzWLFlpKgI8WsHaWVZKZG9tYRqCiISAvr1C6Zx64YA8Blj8xg5sptASeKTWu2l9KjfVrEz6uiICItrnendB6+LJ+yymrOvf8TLntkBvPW7gg6VkxZs62Mnh1UFESklTikZ3ve/8mx3HLqQcxdu4MJ93yk0duaaFdVDesLy8hrTUXBzFLMbIaZzTWzBWb2m0b2udzMCsxsTmj6brjyiEjkpSbF872j+/HBzcdy+fjePDVjtUZva4J1O8pwJ5ArhXC2YlQAx7l7sZklAh+a2evuPq3Bfs+4+3VhzCEiActISeRXpw9m+ZYSbn1lIaN7tefgrplBx4paq7eVAsEUhbBdKXit4tBiYmjSdaNIGxUXZ/z1vBFkpdaO3la6qyroSFGrVRYFADOLN7M5wGbgbXef3shuZ5vZPDN71szywplHRILVqV0yd5w/kuVbSvj15AVBx4laa7aVkpQQR25GcsTPHdai4O7V7j4S6AGMMbOhDXZ5Gejt7sOBKcBjjR3HzCaa2Swzm1VQUBDOyCISZof378S1x/TnX7PW8sxMtS80ZvXWUvLapxIXZxE/d0SePnL3HcBU4JQG67e6e0Vo8UFg9B4+P8nd8909PycnJ6xZRST8bjxhAEcO6MTPX5jP2ws3BR0n6qzZXhrIrSMI79NHOWaWHZpPBU4AvmiwT9d6ixOAReHKIyLRIyE+jvsvHs3Q7llc++SnTFu+NehIUcPdWb21FRYFoCvwrpnNA2ZS26bwipndamYTQvtcH3pcdS5wPXB5GPOISBRJT07g0csPJa99Klc9Nov56wqDjhQVCssqKaqoCuQdBQjv00fz3P0Qdx/u7kPd/dbQ+l+5++TQ/C3uPsTdR7j7se7+xd6PKiKtSYf0JP5x5VgyUhK47JEZzF6lLjGCfPII9EaziASsW3YqT3x3LO1SErhg0jT+OX1Vm37rua4odFRREJE2qm9OOyZfewSH9+/EL16Yzy3Pf05FVXXQsQKxuyjkBdAZHqgoiEiUyEpL5OHLDuW6Y/vz9Mw1THx8NtU1be+KYc22Ujq1S4r4MJy7qSiISNSIjzN+fPIgfnfmUN5bXMAdUxYHHSniVm8rDayRGVQURCQKXTyuF+fn53H3f5YypY29x7B6W3CPo4KKgohEqd+cMYRh3bP44b/msLKNjOBWWV3D+h3lKgoiIg2lJMbzt2+PIj7OuPqJ2RSWVgYdKew27CinusZ1+0hEpDF5HdK484JDWLK5mONum8qzs9e26sdVg35HAVQURCTKHT0wh5euPZyeHdP48b/nct4Dn/DFxp1BxwqLusdRVRRERPZsaPcsnrt6PH8+ezjLCkr45t0f8tAHy1vdVcPqbaUkxhtdMlMCy6CiICIxIS7OOO/QPN656WiOGZTL715dxJWPzWJbya6go7WYNdtL6dE+jfgAuszeTUVBRGJK+/QkJl0yml9/czAfLtnCqXe+32p6WV0T8DsKoKIgIjHIzLj88D48//3xpCclcNGD07hzypKYfwO69h2F1EAzqCiISMwa2j2LyT84ggkjunH7lMVc/NB0Nu8sDzpWk9XUOOWV1ZRXVrO1uIIdpZWBPnkEEEznGiIiLaRdcgK3nz+S8f078auX5nPaXR/wjyvHcnDXzKCj7VVNjXPWfR8zZ82Or6zv2SE9oES1VBREJOaZGefl53FIXjaXPjKDSx6ezjPfO4x+Oe2CjrZHbyzYyJw1O7hobE96tE/FHZIT4jhmULBDDlusPdKVn5/vs2bNCjqGiESpZQXFnP/AJyTExfHvqw8LvOG2MTU1zil3vk+Nw5s3HhWRp43MbLa75+9rP7UpiEir0i+nHf+4cixlldVc9NA0NhZGXxvDq59vYPGmYq4/fkCgj582RkVBRFqdg7tm8vh3xrC9pJKz7/uY6VH0yGp1jXPnO0sYkNuObwzrGnScr1FREJFWaUReNv/87lgS4o0LHpzGH15fFBWjub0ybz1LNxdzwwnRd5UAKgoi0oqNyMvmteuP5IJDe/LAe8s5896PWbKpKLA81TXOXe8sYVDnDE4bGn1XCaCiICKtXHpyAn84axgPX5ZPQVE537znQ/41a00g/Sa9Mm89ywpKuPGEAcRF4VUCqCiISBtx/MEJNhfgAAAKoklEQVSdee36IxnVsz03PzuPm/41l+KKqoid3925/73lDMhtx8lDukTsvM0VtqJgZilmNsPM5prZAjP7TSP7JJvZM2a21Mymm1nvcOUREcnNTOEfV47lphMH8tKcdUy450NWbY3MqG4fLt3Cog07ueqovlF7lQDhvVKoAI5z9xHASOAUMxvXYJ8rge3u3h+4HfhTGPOIiBAfZ1x//ACevGoc20t2cdbfvv5WcThMen85ORnJnDGyW9jPdSDCVhS8VnFoMTE0NbyJdwbwWGj+WeB4M4veEioirca4vh157prxpCXHc+GkabyzaFPYzrVow04+WLKFy8f3JjkhPmznaQlhbVMws3gzmwNsBt529+kNdukOrAFw9yqgEOjYyHEmmtksM5tVUFAQzsgi0ob0zWnH89ccTv/cdlz1+Cz+MW1VWM7z4PvLSUuK5+KxvcJy/JYU1qLg7tXuPhLoAYwxs6ENdmnsquBrjwS4+yR3z3f3/JycYPsFEZHWJScjmacnjuOYQbn88sX53PL8vBZ9n2FDYRmT567n/EPzyEpLbLHjhktEnj5y9x3AVOCUBpvWAnkAZpYAZAHbIpFJRGS39OQEHrw0n2uP7cdTM9ZwwaRpbGqhLrj//tFKHPjO4X1a5HjhFs6nj3LMLDs0nwqcAHzRYLfJwGWh+XOA/3is9dAnIq1CfJzxk5MP4r5vj+LLjUWcfveHvPvl5gM65vx1hTw5fTWnDesalR3zNSacVwpdgXfNbB4wk9o2hVfM7FYzmxDa52Ggo5ktBW4CfhbGPCIi+3TqsK68eO3hZKcmcsWjM/nhM3P2axzo1z/fwLn3f0JGSgI3nTgwDEnDQ11ni4g0oqKqmnv/s5S/TV1GVmoiv/rmYCaM6Ma+HpB0d+56Zym3T1nMIT2zeeCS0eRmpEQo9Z6p62wRkQOQnBDPTScN4uUfHEH39qnc8PQczrrvY2av2nOzZ0VVNTc8PYfbpyzmrFHdeeqqcVFREJpDRUFEZC8O7prJC98/nD+fPZx128s4+75P+P4/Z7OsoPgr+xWVV3LFozOZPHc9N58yiNvOHUFKYnS/k9AY3T4SEWmi0l1VPPj+Ch54fxllldWcPrwb1x3bn/ZpiVz26EyWbCriz+cM56xRPYKO+jVNvX2koiAi0kxbiit46IMV/OOTlZTsqiYrNZHK6hruu3g0Rw+MznepVBRERMJsR+kuHv1oJVMXF/DbM4YwvEd20JH2SEVBRETq6OkjERFpNhUFERGpo6IgIiJ1VBRERKSOioKIiNRRURARkToqCiIiUkdFQURE6sTcy2tmVgDsHkg1i9pxnWnC8u75xtZ1ArY0M0rDczV1e2PrG8u0p/kDyby3XE3NFyuZG1sfi9+PpmSuP6/vR9O3t7XvR7a777sPDneP2QmY1NTl3fN7WDfrQM/d1O2NrW8s077y70/m/c0di5lby/ejKZmD/lnr+xE734+mTLF+++jlZiy/vJd1LXHupm5vbP2eMu0r//7Yn9yxmLmx9bH4/WhK5vrz+n40fXtb/H7sU8zdPgoHM5vlTegTJJooc+TEYm5ljpxYzb0nsX6l0FImBR1gPyhz5MRibmWOnFjN3ShdKYiISB1dKYiISB0VBRERqaOiICIidVQU9sHM4szs92Z2t5ldFnSepjCzY8zsAzO738yOCTpPU5lZupnNNrPTg87SFGZ2cOhn/KyZXRN0nqYyszPN7EEze8nMTgo6T1OYWV8ze9jMng06y96EvsOPhX6+3w46z/5o1UXBzB4xs81mNr/B+lPM7EszW2pmP9vHYc4AugOVwNpwZa2XrSUyO1AMpBA7mQF+CvwrPCm/qiUyu/sid78aOA+IyCOJLZT7RXe/CrgcOD+McXdna4nMy939yvAmbVwz858FPBv6+U6IeNiWsD9vEMbKBBwFjALm11sXDywD+gJJwFxgMDAMeKXBlAv8DPhe6LPPxkjmuNDnOgP/jJHMJwAXUPuL6vRYyBz6zATgY+CiWPlO1/vcbcCoGMsc9n+DB5j/FmBkaJ8nI521JaYEWjF3f9/MejdYPQZY6u7LAczsaeAMd/8D8LXbFma2FtgVWqwOX9paLZG5nu1Acjhy1tdCP+djgXRq/2GVmdlr7l4TzZlDx5kMTDazV4Enw5W33vla4mdtwB+B19390/AmbvHvdMQ1Jz+1V+Y9gDnE6J2YVl0U9qA7sKbe8lpg7F72fx6428yOBN4PZ7C9aFZmMzsLOBnIBu4Jb7Q9alZmd/8FgJldDmwJZ0HYi+b+nI+h9nZBMvBaWJPtXXO/0z+g9sosy8z6u/v94Qy3B839WXcEfg8cYma3hIpHkPaU/y7gHjP7BgfefUcg2mJRsEbW7fENPncvBQK5l1lPczM/T20xC1KzMtft4P73lo/SZM39OU8FpoYrTDM0N/dd1P7yClJzM28Frg5fnGZrNL+7lwBXRDpMS4rJy5sDtBbIq7fcA1gfUJamUubIiMXMEJu5YzFzfbGef4/aYlGYCQwwsz5mlkRt4+bkgDPtizJHRixmhtjMHYuZ64v1/HsWdEt3OCfgKWAD/32c9MrQ+tOAxdQ+PfCLoHMqszK35tyxmLk15W/upA7xRESkTlu8fSQiInugoiAiInVUFEREpI6KgoiI1FFREBGROioKIiJSR0VBWg0zK47w+R4ys8ERPueNZpYWyXNK26L3FKTVMLNid2/XgsdLcPeqljpeE89p1P67bLRDQDNbCeS7+5ZI5pK2Q1cK0qqZWY6ZPWdmM0PT4aH1Y8zsYzP7LPTfQaH1l5vZv83sZeAtqx3FbqrVjq72hZn9M/SLm9D6/NB8sdWO0DfXzKaZWefQ+n6h5ZlmdmtjVzNm1tvMFpnZ34BPgTwzu8/MZpnZAjP7TWi/64FuwLtm9m5o3Ulm9omZfRrK3WJFUdqooF+p1qSppSaguJF1TwJHhOZ7AotC85lAQmj+BOC50Pzl1HZl0CG0fAxQSG2HZ3HAJ/WON5Xav9qhtofPb4bm/wz8T2j+FeDC0PzVe8jYG6gBxtVbt/v88aHzDA8trwQ6heY7Udude3po+afAr4L+/0FTbE9tsetsaVtOAAaH/rgHyDSzDCALeMzMBlD7Cz2x3mfedvdt9ZZnuPtaADObQ+0v8Q8bnGcXtQUAYDZwYmj+MODM0PyTwF/2kHOVu0+rt3yemU2ktnv7rtQOPjSvwWfGhdZ/FPrfl0Rt0RLZbyoK0trFAYe5e1n9lWZ2N/Cuu38rNKrW1HqbSxoco6LefDWN/7updHffxz57U3dOM+sD/Bg41N23m9nfqR1vuyGjtoBd2MxzieyR2hSktXsLuG73gpmNDM1mAetC85eH8fzTgLND8xc08TOZ1BaJwlDbxKn1thUBGfWOfbiZ9QcwszQzG3jgkaUtU1GQ1iTNzNbWm24CrgfyzWyemS3kv6N3/Rn4g5l9RO19+3C5EbjJzGZQexuocF8fcPe5wGfAAuAR4KN6mycBr5vZu+5eQG1Be8rM5lFbJA5q2fjS1uiRVJEwCr1TUObubmYXUNvofEbQuUT2RG0KIuE1mtqB3A3YAXwn4Dwie6UrBRERqaM2BRERqaOiICIidVQURESkjoqCiIjUUVEQEZE6KgoiIlLn/wNq/+U3p/HwIQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Parameters\n",
    "params = {'batch_size': 128,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 8}\n",
    "\n",
    "model = DenseNet().to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-7, eps=1e-8, weight_decay=0.0001)\n",
    "criterion = nn.L1Loss()\n",
    "train_dataloader_lr = data.DataLoader(LANL_Dataset_LR(train_df), **params)\n",
    "\n",
    "lr_find = LRFinder(model, optimizer, criterion)\n",
    "lr_find.range_test(train_dataloader_lr)\n",
    "lr_find.plot()\n",
    "lr_find.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LANL_train(model,\n",
    "               dataloaders,\n",
    "               optimizer,\n",
    "               criterion=nn.L1Loss(),\n",
    "               num_epochs=1000,\n",
    "               patience=1000,\n",
    "               snapshot_path='./snapshots',\n",
    "               model_path='./models'):\n",
    "    \n",
    "    train_loss_hist = []\n",
    "    valid_loss_hist = []\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = -1\n",
    "    \n",
    "    early_stopping = False\n",
    "    early_stopping_count = 0\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.99, last_epoch=-1)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        if not early_stopping:\n",
    "            print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "            print('-' * 10)\n",
    "            scheduler.step()\n",
    "\n",
    "            for phase in ['train', 'valid']:\n",
    "                if phase == 'train':\n",
    "                    model.train()\n",
    "                if phase == 'valid':\n",
    "                    model.eval()\n",
    "\n",
    "                running_loss = 0.0\n",
    "                \n",
    "                for idx, sample in enumerate(dataloaders[phase]):\n",
    "                    X, y = sample['X'].float().to(device), sample['y'].float().to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        output = model(X)\n",
    "                        loss = criterion(output, y)\n",
    "\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    running_loss += loss.item() * X.size(0)\n",
    "\n",
    "                epoch_loss = running_loss/len(dataloaders[phase].dataset)\n",
    "\n",
    "                print('{} loss: {:.6f}'.format(phase, epoch_loss))\n",
    "\n",
    "                if phase == 'train':\n",
    "                    train_loss_hist.append(epoch_loss)\n",
    "                if phase == 'valid':\n",
    "                    valid_loss_hist.append(epoch_loss)\n",
    "                    datetime_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "                    torch.save(copy.deepcopy(model.state_dict()), snapshot_path + '/snapshot_' + datetime_str + '_' + str(epoch_loss) + '.pt')\n",
    "                    for param_group in optimizer.param_groups:\n",
    "                        print(param_group['lr'])\n",
    "                    if epoch_loss < best_loss or best_loss == -1:\n",
    "                        early_stopping_count = 0\n",
    "                        best_loss = epoch_loss\n",
    "                        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    else:\n",
    "                        early_stopping_count += 1\n",
    "                        print('Early stopping count at {} of {}'.format(early_stopping_count, patience))\n",
    "                        if early_stopping_count >= patience:\n",
    "                            early_stopping = True\n",
    "                            print('Out of patience, early stopping training')\n",
    "            \n",
    "            print()\n",
    "\n",
    "    print('Best validation loss: {:6f}'.format(best_loss))\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    datetime_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    torch.save(best_model_wts, model_path + '/model_' + datetime_str + '_' + str(best_loss) + '.pt')\n",
    "    \n",
    "    return model, train_loss_hist, valid_loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {'batch_size': 128,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 8}\n",
    "\n",
    "datasets = {'train': LANL_Dataset(train_df),\n",
    "            'valid': LANL_Dataset(valid_df),\n",
    "            'test' : LANL_Dataset(test_df )}\n",
    "\n",
    "dataloaders = {phase: data.DataLoader(dataset, **params) for phase, dataset in datasets.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "----------\n",
      "train loss: 8.907161\n",
      "valid loss: 19639.851684\n",
      "0.5\n",
      "\n",
      "Epoch 2/1000\n",
      "----------\n",
      "train loss: 3.427588\n",
      "valid loss: 61.469513\n",
      "0.495\n",
      "\n",
      "Epoch 3/1000\n",
      "----------\n",
      "train loss: 3.239196\n",
      "valid loss: 52.572350\n",
      "0.49005\n",
      "\n",
      "Epoch 4/1000\n",
      "----------\n",
      "train loss: 2.504811\n",
      "valid loss: 3.677550\n",
      "0.4851495\n",
      "\n",
      "Epoch 5/1000\n",
      "----------\n",
      "train loss: 2.507262\n",
      "valid loss: 2.567552\n",
      "0.480298005\n",
      "\n",
      "Epoch 6/1000\n",
      "----------\n",
      "train loss: 2.247999\n",
      "valid loss: 2.897195\n",
      "0.47549502494999996\n",
      "Early stopping count at 1 of 1000\n",
      "\n",
      "Epoch 7/1000\n",
      "----------\n",
      "train loss: 2.231349\n",
      "valid loss: 2.981246\n",
      "0.4707400747005\n",
      "Early stopping count at 2 of 1000\n",
      "\n",
      "Epoch 8/1000\n",
      "----------\n",
      "train loss: 2.124702\n",
      "valid loss: 2.476452\n",
      "0.46603267395349496\n",
      "\n",
      "Epoch 9/1000\n",
      "----------\n",
      "train loss: 2.154244\n",
      "valid loss: 3.076987\n",
      "0.46137234721396003\n",
      "Early stopping count at 1 of 1000\n",
      "\n",
      "Epoch 10/1000\n",
      "----------\n",
      "train loss: 2.142127\n",
      "valid loss: 3.076929\n",
      "0.4567586237418204\n",
      "Early stopping count at 2 of 1000\n",
      "\n",
      "Epoch 11/1000\n",
      "----------\n",
      "train loss: 2.104325\n",
      "valid loss: 2.280865\n",
      "0.4521910375044022\n",
      "\n",
      "Epoch 12/1000\n",
      "----------\n",
      "train loss: 2.111165\n",
      "valid loss: 4.664354\n",
      "0.4476691271293582\n",
      "Early stopping count at 1 of 1000\n",
      "\n",
      "Epoch 13/1000\n",
      "----------\n",
      "train loss: 2.107894\n",
      "valid loss: 2.423797\n",
      "0.4431924358580646\n",
      "Early stopping count at 2 of 1000\n",
      "\n",
      "Epoch 14/1000\n",
      "----------\n",
      "train loss: 2.094240\n",
      "valid loss: 2.901301\n",
      "0.4387605114994839\n",
      "Early stopping count at 3 of 1000\n",
      "\n",
      "Epoch 15/1000\n",
      "----------\n",
      "train loss: 2.058126\n",
      "valid loss: 5.645116\n",
      "0.4343729063844891\n",
      "Early stopping count at 4 of 1000\n",
      "\n",
      "Epoch 16/1000\n",
      "----------\n",
      "train loss: 2.048991\n",
      "valid loss: 4.204667\n",
      "0.4300291773206442\n",
      "Early stopping count at 5 of 1000\n",
      "\n",
      "Epoch 17/1000\n",
      "----------\n",
      "train loss: 2.060351\n",
      "valid loss: 2.322604\n",
      "0.42572888554743776\n",
      "Early stopping count at 6 of 1000\n",
      "\n",
      "Epoch 18/1000\n",
      "----------\n",
      "train loss: 2.140778\n",
      "valid loss: 2.598061\n",
      "0.4214715966919634\n",
      "Early stopping count at 7 of 1000\n",
      "\n",
      "Epoch 19/1000\n",
      "----------\n",
      "train loss: 2.043132\n",
      "valid loss: 2.186283\n",
      "0.41725688072504374\n",
      "\n",
      "Epoch 20/1000\n",
      "----------\n",
      "train loss: 2.033172\n",
      "valid loss: 3.019290\n",
      "0.4130843119177933\n",
      "Early stopping count at 1 of 1000\n",
      "\n",
      "Epoch 21/1000\n",
      "----------\n",
      "train loss: 2.025729\n",
      "valid loss: 2.411025\n",
      "0.4089534687986154\n",
      "Early stopping count at 2 of 1000\n",
      "\n",
      "Epoch 22/1000\n",
      "----------\n",
      "train loss: 2.062445\n",
      "valid loss: 2.729297\n",
      "0.4048639341106292\n",
      "Early stopping count at 3 of 1000\n",
      "\n",
      "Epoch 23/1000\n",
      "----------\n",
      "train loss: 2.053558\n",
      "valid loss: 3.225285\n",
      "0.40081529476952293\n",
      "Early stopping count at 4 of 1000\n",
      "\n",
      "Epoch 24/1000\n",
      "----------\n",
      "train loss: 2.028260\n",
      "valid loss: 2.218311\n",
      "0.3968071418218277\n",
      "Early stopping count at 5 of 1000\n",
      "\n",
      "Epoch 25/1000\n",
      "----------\n",
      "train loss: 2.085455\n",
      "valid loss: 2.455748\n",
      "0.3928390704036094\n",
      "Early stopping count at 6 of 1000\n",
      "\n",
      "Epoch 26/1000\n",
      "----------\n",
      "train loss: 2.019324\n",
      "valid loss: 2.401040\n",
      "0.3889106796995733\n",
      "Early stopping count at 7 of 1000\n",
      "\n",
      "Epoch 27/1000\n",
      "----------\n",
      "train loss: 2.004502\n",
      "valid loss: 3.261500\n",
      "0.38502157290257755\n",
      "Early stopping count at 8 of 1000\n",
      "\n",
      "Epoch 28/1000\n",
      "----------\n",
      "train loss: 1.999181\n",
      "valid loss: 2.240621\n",
      "0.3811713571735518\n",
      "Early stopping count at 9 of 1000\n",
      "\n",
      "Epoch 29/1000\n",
      "----------\n",
      "train loss: 2.014494\n",
      "valid loss: 2.252931\n",
      "0.3773596436018163\n",
      "Early stopping count at 10 of 1000\n",
      "\n",
      "Epoch 30/1000\n",
      "----------\n",
      "train loss: 2.003153\n",
      "valid loss: 3.903088\n",
      "0.3735860471657981\n",
      "Early stopping count at 11 of 1000\n",
      "\n",
      "Epoch 31/1000\n",
      "----------\n",
      "train loss: 2.072243\n",
      "valid loss: 2.726005\n",
      "0.3698501866941401\n",
      "Early stopping count at 12 of 1000\n",
      "\n",
      "Epoch 32/1000\n",
      "----------\n",
      "train loss: 2.035730\n",
      "valid loss: 2.463246\n",
      "0.36615168482719873\n",
      "Early stopping count at 13 of 1000\n",
      "\n",
      "Epoch 33/1000\n",
      "----------\n",
      "train loss: 2.031133\n",
      "valid loss: 3.652663\n",
      "0.3624901679789267\n",
      "Early stopping count at 14 of 1000\n",
      "\n",
      "Epoch 34/1000\n",
      "----------\n",
      "train loss: 2.032804\n",
      "valid loss: 3.070105\n",
      "0.35886526629913745\n",
      "Early stopping count at 15 of 1000\n",
      "\n",
      "Epoch 35/1000\n",
      "----------\n",
      "train loss: 2.041666\n",
      "valid loss: 2.903189\n",
      "0.35527661363614604\n",
      "Early stopping count at 16 of 1000\n",
      "\n",
      "Epoch 36/1000\n",
      "----------\n",
      "train loss: 2.036876\n",
      "valid loss: 4.869167\n",
      "0.3517238474997846\n",
      "Early stopping count at 17 of 1000\n",
      "\n",
      "Epoch 37/1000\n",
      "----------\n",
      "train loss: 1.950213\n",
      "valid loss: 2.262937\n",
      "0.34820660902478673\n",
      "Early stopping count at 18 of 1000\n",
      "\n",
      "Epoch 38/1000\n",
      "----------\n",
      "train loss: 2.000484\n",
      "valid loss: 3.676015\n",
      "0.34472454293453886\n",
      "Early stopping count at 19 of 1000\n",
      "\n",
      "Epoch 39/1000\n",
      "----------\n",
      "train loss: 1.930921\n",
      "valid loss: 3.551786\n",
      "0.3412772975051935\n",
      "Early stopping count at 20 of 1000\n",
      "\n",
      "Epoch 40/1000\n",
      "----------\n",
      "train loss: 1.904286\n",
      "valid loss: 3.061943\n",
      "0.33786452453014154\n",
      "Early stopping count at 21 of 1000\n",
      "\n",
      "Epoch 41/1000\n",
      "----------\n",
      "train loss: 1.914022\n",
      "valid loss: 4.536852\n",
      "0.33448587928484014\n",
      "Early stopping count at 22 of 1000\n",
      "\n",
      "Epoch 42/1000\n",
      "----------\n",
      "train loss: 1.970566\n",
      "valid loss: 2.745150\n",
      "0.33114102049199173\n",
      "Early stopping count at 23 of 1000\n",
      "\n",
      "Epoch 43/1000\n",
      "----------\n",
      "train loss: 1.931647\n",
      "valid loss: 2.758660\n",
      "0.3278296102870718\n",
      "Early stopping count at 24 of 1000\n",
      "\n",
      "Epoch 44/1000\n",
      "----------\n",
      "train loss: 1.938857\n",
      "valid loss: 2.651086\n",
      "0.3245513141842011\n",
      "Early stopping count at 25 of 1000\n",
      "\n",
      "Epoch 45/1000\n",
      "----------\n",
      "train loss: 1.958945\n",
      "valid loss: 4.052741\n",
      "0.32130580104235906\n",
      "Early stopping count at 26 of 1000\n",
      "\n",
      "Epoch 46/1000\n",
      "----------\n",
      "train loss: 1.890227\n",
      "valid loss: 4.487465\n",
      "0.31809274303193547\n",
      "Early stopping count at 27 of 1000\n",
      "\n",
      "Epoch 47/1000\n",
      "----------\n",
      "train loss: 1.859725\n",
      "valid loss: 3.702205\n",
      "0.31491181560161613\n",
      "Early stopping count at 28 of 1000\n",
      "\n",
      "Epoch 48/1000\n",
      "----------\n",
      "train loss: 1.894703\n",
      "valid loss: 2.732651\n",
      "0.3117626974456\n",
      "Early stopping count at 29 of 1000\n",
      "\n",
      "Epoch 49/1000\n",
      "----------\n",
      "train loss: 2.129518\n",
      "valid loss: 6.837415\n",
      "0.308645070471144\n",
      "Early stopping count at 30 of 1000\n",
      "\n",
      "Epoch 50/1000\n",
      "----------\n",
      "train loss: 1.870584\n",
      "valid loss: 3.694312\n",
      "0.3055586197664325\n",
      "Early stopping count at 31 of 1000\n",
      "\n",
      "Epoch 51/1000\n",
      "----------\n",
      "train loss: 1.876720\n",
      "valid loss: 2.937044\n",
      "0.3025030335687682\n",
      "Early stopping count at 32 of 1000\n",
      "\n",
      "Epoch 52/1000\n",
      "----------\n",
      "train loss: 1.857942\n",
      "valid loss: 4.105147\n",
      "0.2994780032330805\n",
      "Early stopping count at 33 of 1000\n",
      "\n",
      "Epoch 53/1000\n",
      "----------\n",
      "train loss: 1.878682\n",
      "valid loss: 4.177225\n",
      "0.2964832232007497\n",
      "Early stopping count at 34 of 1000\n",
      "\n",
      "Epoch 54/1000\n",
      "----------\n",
      "train loss: 1.859200\n",
      "valid loss: 3.530235\n",
      "0.2935183909687422\n",
      "Early stopping count at 35 of 1000\n",
      "\n",
      "Epoch 55/1000\n",
      "----------\n",
      "train loss: 1.857370\n",
      "valid loss: 3.180750\n",
      "0.29058320705905477\n",
      "Early stopping count at 36 of 1000\n",
      "\n",
      "Epoch 56/1000\n",
      "----------\n",
      "train loss: 1.912994\n",
      "valid loss: 2.693795\n",
      "0.28767737498846424\n",
      "Early stopping count at 37 of 1000\n",
      "\n",
      "Epoch 57/1000\n",
      "----------\n",
      "train loss: 1.834209\n",
      "valid loss: 3.743135\n",
      "0.2848006012385796\n",
      "Early stopping count at 38 of 1000\n",
      "\n",
      "Epoch 58/1000\n",
      "----------\n",
      "train loss: 1.826046\n",
      "valid loss: 2.562749\n",
      "0.28195259522619376\n",
      "Early stopping count at 39 of 1000\n",
      "\n",
      "Epoch 59/1000\n",
      "----------\n",
      "train loss: 1.825762\n",
      "valid loss: 4.897300\n",
      "0.27913306927393183\n",
      "Early stopping count at 40 of 1000\n",
      "\n",
      "Epoch 60/1000\n",
      "----------\n",
      "train loss: 1.886068\n",
      "valid loss: 2.876842\n",
      "0.27634173858119254\n",
      "Early stopping count at 41 of 1000\n",
      "\n",
      "Epoch 61/1000\n",
      "----------\n",
      "train loss: 1.834419\n",
      "valid loss: 3.801349\n",
      "0.2735783211953806\n",
      "Early stopping count at 42 of 1000\n",
      "\n",
      "Epoch 62/1000\n",
      "----------\n",
      "train loss: 1.820929\n",
      "valid loss: 3.045888\n",
      "0.2708425379834268\n",
      "Early stopping count at 43 of 1000\n",
      "\n",
      "Epoch 63/1000\n",
      "----------\n",
      "train loss: 1.827271\n",
      "valid loss: 5.782596\n",
      "0.2681341126035925\n",
      "Early stopping count at 44 of 1000\n",
      "\n",
      "Epoch 64/1000\n",
      "----------\n",
      "train loss: 1.837136\n",
      "valid loss: 3.165190\n",
      "0.2654527714775566\n",
      "Early stopping count at 45 of 1000\n",
      "\n",
      "Epoch 65/1000\n",
      "----------\n",
      "train loss: 1.942672\n",
      "valid loss: 3.025763\n",
      "0.262798243762781\n",
      "Early stopping count at 46 of 1000\n",
      "\n",
      "Epoch 66/1000\n",
      "----------\n",
      "train loss: 1.854877\n",
      "valid loss: 2.811543\n",
      "0.2601702613251532\n",
      "Early stopping count at 47 of 1000\n",
      "\n",
      "Epoch 67/1000\n",
      "----------\n",
      "train loss: 1.819002\n",
      "valid loss: 2.649790\n",
      "0.25756855871190165\n",
      "Early stopping count at 48 of 1000\n",
      "\n",
      "Epoch 68/1000\n",
      "----------\n",
      "train loss: 1.806392\n",
      "valid loss: 4.725378\n",
      "0.25499287312478264\n",
      "Early stopping count at 49 of 1000\n",
      "\n",
      "Epoch 69/1000\n",
      "----------\n",
      "train loss: 1.786339\n",
      "valid loss: 8.415420\n",
      "0.2524429443935348\n",
      "Early stopping count at 50 of 1000\n",
      "\n",
      "Epoch 70/1000\n",
      "----------\n",
      "train loss: 1.828199\n",
      "valid loss: 4.646185\n",
      "0.24991851494959946\n",
      "Early stopping count at 51 of 1000\n",
      "\n",
      "Epoch 71/1000\n",
      "----------\n",
      "train loss: 1.795030\n",
      "valid loss: 3.516328\n",
      "0.24741932980010348\n",
      "Early stopping count at 52 of 1000\n",
      "\n",
      "Epoch 72/1000\n",
      "----------\n",
      "train loss: 1.878062\n",
      "valid loss: 2.737349\n",
      "0.24494513650210245\n",
      "Early stopping count at 53 of 1000\n",
      "\n",
      "Epoch 73/1000\n",
      "----------\n",
      "train loss: 1.781872\n",
      "valid loss: 7.075616\n",
      "0.24249568513708142\n",
      "Early stopping count at 54 of 1000\n",
      "\n",
      "Epoch 74/1000\n",
      "----------\n",
      "train loss: 1.762542\n",
      "valid loss: 4.958499\n",
      "0.2400707282857106\n",
      "Early stopping count at 55 of 1000\n",
      "\n",
      "Epoch 75/1000\n",
      "----------\n",
      "train loss: 1.753961\n",
      "valid loss: 5.395330\n",
      "0.23767002100285348\n",
      "Early stopping count at 56 of 1000\n",
      "\n",
      "Epoch 76/1000\n",
      "----------\n",
      "train loss: 1.747253\n",
      "valid loss: 3.078214\n",
      "0.23529332079282494\n",
      "Early stopping count at 57 of 1000\n",
      "\n",
      "Epoch 77/1000\n",
      "----------\n",
      "train loss: 1.848732\n",
      "valid loss: 3.104205\n",
      "0.23294038758489669\n",
      "Early stopping count at 58 of 1000\n",
      "\n",
      "Epoch 78/1000\n",
      "----------\n",
      "train loss: 1.811753\n",
      "valid loss: 3.645262\n",
      "0.23061098370904773\n",
      "Early stopping count at 59 of 1000\n",
      "\n",
      "Epoch 79/1000\n",
      "----------\n",
      "train loss: 1.783591\n",
      "valid loss: 6.140121\n",
      "0.22830487387195725\n",
      "Early stopping count at 60 of 1000\n",
      "\n",
      "Epoch 80/1000\n",
      "----------\n",
      "train loss: 1.742476\n",
      "valid loss: 4.943153\n",
      "0.22602182513323768\n",
      "Early stopping count at 61 of 1000\n",
      "\n",
      "Epoch 81/1000\n",
      "----------\n",
      "train loss: 1.818490\n",
      "valid loss: 4.408653\n",
      "0.2237616068819053\n",
      "Early stopping count at 62 of 1000\n",
      "\n",
      "Epoch 82/1000\n",
      "----------\n",
      "train loss: 1.771766\n",
      "valid loss: 5.638615\n",
      "0.22152399081308624\n",
      "Early stopping count at 63 of 1000\n",
      "\n",
      "Epoch 83/1000\n",
      "----------\n",
      "train loss: 1.743744\n",
      "valid loss: 6.228550\n",
      "0.21930875090495539\n",
      "Early stopping count at 64 of 1000\n",
      "\n",
      "Epoch 84/1000\n",
      "----------\n",
      "train loss: 1.725277\n",
      "valid loss: 6.250865\n",
      "0.21711566339590582\n",
      "Early stopping count at 65 of 1000\n",
      "\n",
      "Epoch 85/1000\n",
      "----------\n",
      "train loss: 1.723172\n",
      "valid loss: 3.252257\n",
      "0.21494450676194676\n",
      "Early stopping count at 66 of 1000\n",
      "\n",
      "Epoch 86/1000\n",
      "----------\n",
      "train loss: 1.735573\n",
      "valid loss: 6.180101\n",
      "0.2127950616943273\n",
      "Early stopping count at 67 of 1000\n",
      "\n",
      "Epoch 87/1000\n",
      "----------\n",
      "train loss: 3.595403\n",
      "valid loss: 3.221868\n",
      "0.210667111077384\n",
      "Early stopping count at 68 of 1000\n",
      "\n",
      "Epoch 88/1000\n",
      "----------\n",
      "train loss: 1.899573\n",
      "valid loss: 3.090431\n",
      "0.20856043996661017\n",
      "Early stopping count at 69 of 1000\n",
      "\n",
      "Epoch 89/1000\n",
      "----------\n",
      "train loss: 1.794598\n",
      "valid loss: 4.667032\n",
      "0.20647483556694407\n",
      "Early stopping count at 70 of 1000\n",
      "\n",
      "Epoch 90/1000\n",
      "----------\n",
      "train loss: 1.824118\n",
      "valid loss: 4.508807\n",
      "0.20441008721127463\n",
      "Early stopping count at 71 of 1000\n",
      "\n",
      "Epoch 91/1000\n",
      "----------\n",
      "train loss: 1.755303\n",
      "valid loss: 4.270964\n",
      "0.2023659863391619\n",
      "Early stopping count at 72 of 1000\n",
      "\n",
      "Epoch 92/1000\n",
      "----------\n",
      "train loss: 1.733424\n",
      "valid loss: 3.131382\n",
      "0.20034232647577027\n",
      "Early stopping count at 73 of 1000\n",
      "\n",
      "Epoch 93/1000\n",
      "----------\n",
      "train loss: 1.754346\n",
      "valid loss: 6.453228\n",
      "0.19833890321101255\n",
      "Early stopping count at 74 of 1000\n",
      "\n",
      "Epoch 94/1000\n",
      "----------\n",
      "train loss: 1.792572\n",
      "valid loss: 4.659440\n",
      "0.19635551417890243\n",
      "Early stopping count at 75 of 1000\n",
      "\n",
      "Epoch 95/1000\n",
      "----------\n",
      "train loss: 1.771967\n",
      "valid loss: 2.745084\n",
      "0.1943919590371134\n",
      "Early stopping count at 76 of 1000\n",
      "\n",
      "Epoch 96/1000\n",
      "----------\n",
      "train loss: 1.750465\n",
      "valid loss: 6.320092\n",
      "0.19244803944674227\n",
      "Early stopping count at 77 of 1000\n",
      "\n",
      "Epoch 97/1000\n",
      "----------\n",
      "train loss: 1.732427\n",
      "valid loss: 3.161407\n",
      "0.19052355905227483\n",
      "Early stopping count at 78 of 1000\n",
      "\n",
      "Epoch 98/1000\n",
      "----------\n",
      "train loss: 1.734777\n",
      "valid loss: 3.349138\n",
      "0.18861832346175209\n",
      "Early stopping count at 79 of 1000\n",
      "\n",
      "Epoch 99/1000\n",
      "----------\n",
      "train loss: 1.747196\n",
      "valid loss: 4.985615\n",
      "0.18673214022713458\n",
      "Early stopping count at 80 of 1000\n",
      "\n",
      "Epoch 100/1000\n",
      "----------\n",
      "train loss: 1.717616\n",
      "valid loss: 4.050867\n",
      "0.18486481882486322\n",
      "Early stopping count at 81 of 1000\n",
      "\n",
      "Epoch 101/1000\n",
      "----------\n",
      "train loss: 1.714698\n",
      "valid loss: 3.224372\n",
      "0.1830161706366146\n",
      "Early stopping count at 82 of 1000\n",
      "\n",
      "Epoch 102/1000\n",
      "----------\n",
      "train loss: 1.711055\n",
      "valid loss: 3.279712\n",
      "0.18118600893024844\n",
      "Early stopping count at 83 of 1000\n",
      "\n",
      "Epoch 103/1000\n",
      "----------\n",
      "train loss: 1.692045\n",
      "valid loss: 10.826696\n",
      "0.17937414884094596\n",
      "Early stopping count at 84 of 1000\n",
      "\n",
      "Epoch 104/1000\n",
      "----------\n",
      "train loss: 1.722906\n",
      "valid loss: 3.433477\n",
      "0.1775804073525365\n",
      "Early stopping count at 85 of 1000\n",
      "\n",
      "Epoch 105/1000\n",
      "----------\n",
      "train loss: 1.736997\n",
      "valid loss: 3.955439\n",
      "0.17580460327901112\n",
      "Early stopping count at 86 of 1000\n",
      "\n",
      "Epoch 106/1000\n",
      "----------\n",
      "train loss: 1.725017\n",
      "valid loss: 5.889239\n",
      "0.174046557246221\n",
      "Early stopping count at 87 of 1000\n",
      "\n",
      "Epoch 107/1000\n",
      "----------\n",
      "train loss: 1.744525\n",
      "valid loss: 9.279172\n",
      "0.1723060916737588\n",
      "Early stopping count at 88 of 1000\n",
      "\n",
      "Epoch 108/1000\n",
      "----------\n",
      "train loss: 1.708353\n",
      "valid loss: 3.700821\n",
      "0.17058303075702122\n",
      "Early stopping count at 89 of 1000\n",
      "\n",
      "Epoch 109/1000\n",
      "----------\n",
      "train loss: 1.770323\n",
      "valid loss: 5.186411\n",
      "0.168877200449451\n",
      "Early stopping count at 90 of 1000\n",
      "\n",
      "Epoch 110/1000\n",
      "----------\n",
      "train loss: 1.693096\n",
      "valid loss: 4.330136\n",
      "0.1671884284449565\n",
      "Early stopping count at 91 of 1000\n",
      "\n",
      "Epoch 111/1000\n",
      "----------\n",
      "train loss: 1.680731\n",
      "valid loss: 8.706036\n",
      "0.16551654416050693\n",
      "Early stopping count at 92 of 1000\n",
      "\n",
      "Epoch 112/1000\n",
      "----------\n",
      "train loss: 1.707382\n",
      "valid loss: 9.432943\n",
      "0.16386137871890186\n",
      "Early stopping count at 93 of 1000\n",
      "\n",
      "Epoch 113/1000\n",
      "----------\n",
      "train loss: 1.780151\n",
      "valid loss: 5.572353\n",
      "0.16222276493171284\n",
      "Early stopping count at 94 of 1000\n",
      "\n",
      "Epoch 114/1000\n",
      "----------\n",
      "train loss: 1.702318\n",
      "valid loss: 7.150653\n",
      "0.1606005372823957\n",
      "Early stopping count at 95 of 1000\n",
      "\n",
      "Epoch 115/1000\n",
      "----------\n",
      "train loss: 1.708830\n",
      "valid loss: 8.999798\n",
      "0.15899453190957175\n",
      "Early stopping count at 96 of 1000\n",
      "\n",
      "Epoch 116/1000\n",
      "----------\n",
      "train loss: 1.738777\n",
      "valid loss: 14.320735\n",
      "0.15740458659047601\n",
      "Early stopping count at 97 of 1000\n",
      "\n",
      "Epoch 117/1000\n",
      "----------\n",
      "train loss: 1.719270\n",
      "valid loss: 6.520795\n",
      "0.15583054072457125\n",
      "Early stopping count at 98 of 1000\n",
      "\n",
      "Epoch 118/1000\n",
      "----------\n",
      "train loss: 1.743644\n",
      "valid loss: 12.786962\n",
      "0.15427223531732553\n",
      "Early stopping count at 99 of 1000\n",
      "\n",
      "Epoch 119/1000\n",
      "----------\n",
      "train loss: 1.718751\n",
      "valid loss: 11.868904\n",
      "0.1527295129641523\n",
      "Early stopping count at 100 of 1000\n",
      "\n",
      "Epoch 120/1000\n",
      "----------\n",
      "train loss: 1.730829\n",
      "valid loss: 6.390090\n",
      "0.15120221783451077\n",
      "Early stopping count at 101 of 1000\n",
      "\n",
      "Epoch 121/1000\n",
      "----------\n",
      "train loss: 1.713027\n",
      "valid loss: 5.008155\n",
      "0.14969019565616565\n",
      "Early stopping count at 102 of 1000\n",
      "\n",
      "Epoch 122/1000\n",
      "----------\n",
      "train loss: 1.697403\n",
      "valid loss: 14.430152\n",
      "0.148193293699604\n",
      "Early stopping count at 103 of 1000\n",
      "\n",
      "Epoch 123/1000\n",
      "----------\n",
      "train loss: 1.717225\n",
      "valid loss: 4.612304\n",
      "0.14671136076260796\n",
      "Early stopping count at 104 of 1000\n",
      "\n",
      "Epoch 124/1000\n",
      "----------\n",
      "train loss: 1.715181\n",
      "valid loss: 7.625619\n",
      "0.14524424715498188\n",
      "Early stopping count at 105 of 1000\n",
      "\n",
      "Epoch 125/1000\n",
      "----------\n",
      "train loss: 1.745986\n",
      "valid loss: 6.505509\n",
      "0.14379180468343206\n",
      "Early stopping count at 106 of 1000\n",
      "\n",
      "Epoch 126/1000\n",
      "----------\n",
      "train loss: 1.722393\n",
      "valid loss: 3.967914\n",
      "0.14235388663659773\n",
      "Early stopping count at 107 of 1000\n",
      "\n",
      "Epoch 127/1000\n",
      "----------\n",
      "train loss: 1.710902\n",
      "valid loss: 6.936091\n",
      "0.14093034777023175\n",
      "Early stopping count at 108 of 1000\n",
      "\n",
      "Epoch 128/1000\n",
      "----------\n",
      "train loss: 1.673213\n",
      "valid loss: 23.551861\n",
      "0.13952104429252943\n",
      "Early stopping count at 109 of 1000\n",
      "\n",
      "Epoch 129/1000\n",
      "----------\n",
      "train loss: 1.691541\n",
      "valid loss: 9.558748\n",
      "0.13812583384960414\n",
      "Early stopping count at 110 of 1000\n",
      "\n",
      "Epoch 130/1000\n",
      "----------\n",
      "train loss: 1.739727\n",
      "valid loss: 7.493175\n",
      "0.1367445755111081\n",
      "Early stopping count at 111 of 1000\n",
      "\n",
      "Epoch 131/1000\n",
      "----------\n",
      "train loss: 1.712424\n",
      "valid loss: 11.087036\n",
      "0.13537712975599703\n",
      "Early stopping count at 112 of 1000\n",
      "\n",
      "Epoch 132/1000\n",
      "----------\n",
      "train loss: 1.770569\n",
      "valid loss: 13.942092\n",
      "0.13402335845843705\n",
      "Early stopping count at 113 of 1000\n",
      "\n",
      "Epoch 133/1000\n",
      "----------\n",
      "train loss: 1.709407\n",
      "valid loss: 8.293424\n",
      "0.13268312487385267\n",
      "Early stopping count at 114 of 1000\n",
      "\n",
      "Epoch 134/1000\n",
      "----------\n",
      "train loss: 1.803499\n",
      "valid loss: 7.353642\n",
      "0.13135629362511414\n",
      "Early stopping count at 115 of 1000\n",
      "\n",
      "Epoch 135/1000\n",
      "----------\n",
      "train loss: 1.715851\n",
      "valid loss: 10.560231\n",
      "0.13004273068886302\n",
      "Early stopping count at 116 of 1000\n",
      "\n",
      "Epoch 136/1000\n",
      "----------\n",
      "train loss: 1.713588\n",
      "valid loss: 11.190320\n",
      "0.12874230338197437\n",
      "Early stopping count at 117 of 1000\n",
      "\n",
      "Epoch 137/1000\n",
      "----------\n",
      "train loss: 14.463651\n",
      "valid loss: 27.619587\n",
      "0.12745488034815464\n",
      "Early stopping count at 118 of 1000\n",
      "\n",
      "Epoch 138/1000\n",
      "----------\n",
      "train loss: 1.878237\n",
      "valid loss: 28.705845\n",
      "0.1261803315446731\n",
      "Early stopping count at 119 of 1000\n",
      "\n",
      "Epoch 139/1000\n",
      "----------\n",
      "train loss: 1.838631\n",
      "valid loss: 14.722154\n",
      "0.12491852822922635\n",
      "Early stopping count at 120 of 1000\n",
      "\n",
      "Epoch 140/1000\n",
      "----------\n",
      "train loss: 1.903230\n",
      "valid loss: 13.854455\n",
      "0.12366934294693409\n",
      "Early stopping count at 121 of 1000\n",
      "\n",
      "Epoch 141/1000\n",
      "----------\n",
      "train loss: 1.810467\n",
      "valid loss: 7.518959\n",
      "0.12243264951746474\n",
      "Early stopping count at 122 of 1000\n",
      "\n",
      "Epoch 142/1000\n",
      "----------\n",
      "train loss: 1.807897\n",
      "valid loss: 18.042422\n",
      "0.1212083230222901\n",
      "Early stopping count at 123 of 1000\n",
      "\n",
      "Epoch 143/1000\n",
      "----------\n",
      "train loss: 1.808523\n",
      "valid loss: 17.737740\n",
      "0.1199962397920672\n",
      "Early stopping count at 124 of 1000\n",
      "\n",
      "Epoch 144/1000\n",
      "----------\n",
      "train loss: 1.781903\n",
      "valid loss: 7.900290\n",
      "0.11879627739414651\n",
      "Early stopping count at 125 of 1000\n",
      "\n",
      "Epoch 145/1000\n",
      "----------\n",
      "train loss: 1.827701\n",
      "valid loss: 7.775732\n",
      "0.11760831462020506\n",
      "Early stopping count at 126 of 1000\n",
      "\n",
      "Epoch 146/1000\n",
      "----------\n",
      "train loss: 1.787558\n",
      "valid loss: 14.755130\n",
      "0.116432231474003\n",
      "Early stopping count at 127 of 1000\n",
      "\n",
      "Epoch 147/1000\n",
      "----------\n",
      "train loss: 1.790063\n",
      "valid loss: 23.229520\n",
      "0.11526790915926297\n",
      "Early stopping count at 128 of 1000\n",
      "\n",
      "Epoch 148/1000\n",
      "----------\n",
      "train loss: 1.768741\n",
      "valid loss: 16.393298\n",
      "0.11411523006767034\n",
      "Early stopping count at 129 of 1000\n",
      "\n",
      "Epoch 149/1000\n",
      "----------\n",
      "train loss: 1.810146\n",
      "valid loss: 12.162158\n",
      "0.11297407776699364\n",
      "Early stopping count at 130 of 1000\n",
      "\n",
      "Epoch 150/1000\n",
      "----------\n",
      "train loss: 1.776450\n",
      "valid loss: 18.584454\n",
      "0.1118443369893237\n",
      "Early stopping count at 131 of 1000\n",
      "\n",
      "Epoch 151/1000\n",
      "----------\n",
      "train loss: 1.742160\n",
      "valid loss: 14.314044\n",
      "0.11072589361943046\n",
      "Early stopping count at 132 of 1000\n",
      "\n",
      "Epoch 152/1000\n",
      "----------\n",
      "train loss: 1.805121\n",
      "valid loss: 14.833722\n",
      "0.10961863468323615\n",
      "Early stopping count at 133 of 1000\n",
      "\n",
      "Epoch 153/1000\n",
      "----------\n",
      "train loss: 1.760224\n",
      "valid loss: 26.722059\n",
      "0.10852244833640379\n",
      "Early stopping count at 134 of 1000\n",
      "\n",
      "Epoch 154/1000\n",
      "----------\n",
      "train loss: 1.744946\n",
      "valid loss: 23.639791\n",
      "0.10743722385303975\n",
      "Early stopping count at 135 of 1000\n",
      "\n",
      "Epoch 155/1000\n",
      "----------\n",
      "train loss: 1.797172\n",
      "valid loss: 11.903493\n",
      "0.10636285161450935\n",
      "Early stopping count at 136 of 1000\n",
      "\n",
      "Epoch 156/1000\n",
      "----------\n",
      "train loss: 1.819716\n",
      "valid loss: 67.602260\n",
      "0.10529922309836426\n",
      "Early stopping count at 137 of 1000\n",
      "\n",
      "Epoch 157/1000\n",
      "----------\n",
      "train loss: 1.822437\n",
      "valid loss: 108.100605\n",
      "0.10424623086738062\n",
      "Early stopping count at 138 of 1000\n",
      "\n",
      "Epoch 158/1000\n",
      "----------\n",
      "train loss: 1.814580\n",
      "valid loss: 32.779394\n",
      "0.1032037685587068\n",
      "Early stopping count at 139 of 1000\n",
      "\n",
      "Epoch 159/1000\n",
      "----------\n",
      "train loss: 1.848854\n",
      "valid loss: 45.091849\n",
      "0.10217173087311975\n",
      "Early stopping count at 140 of 1000\n",
      "\n",
      "Epoch 160/1000\n",
      "----------\n",
      "train loss: 1.795005\n",
      "valid loss: 49.574129\n",
      "0.10115001356438855\n",
      "Early stopping count at 141 of 1000\n",
      "\n",
      "Epoch 161/1000\n",
      "----------\n",
      "train loss: 56.294912\n",
      "valid loss: 28.422136\n",
      "0.10013851342874465\n",
      "Early stopping count at 142 of 1000\n",
      "\n",
      "Epoch 162/1000\n",
      "----------\n",
      "train loss: 1.763814\n",
      "valid loss: 8.165372\n",
      "0.09913712829445721\n",
      "Early stopping count at 143 of 1000\n",
      "\n",
      "Epoch 163/1000\n",
      "----------\n",
      "train loss: 1.801666\n",
      "valid loss: 24.931432\n",
      "0.09814575701151264\n",
      "Early stopping count at 144 of 1000\n",
      "\n",
      "Epoch 164/1000\n",
      "----------\n",
      "train loss: 1.773326\n",
      "valid loss: 48.588003\n",
      "0.09716429944139751\n",
      "Early stopping count at 145 of 1000\n",
      "\n",
      "Epoch 165/1000\n",
      "----------\n",
      "train loss: 1.904530\n",
      "valid loss: 39.247784\n",
      "0.09619265644698353\n",
      "Early stopping count at 146 of 1000\n",
      "\n",
      "Epoch 166/1000\n",
      "----------\n",
      "train loss: 1.810348\n",
      "valid loss: 22.638337\n",
      "0.0952307298825137\n",
      "Early stopping count at 147 of 1000\n",
      "\n",
      "Epoch 167/1000\n",
      "----------\n",
      "train loss: 2.622079\n",
      "valid loss: 116.845554\n",
      "0.09427842258368856\n",
      "Early stopping count at 148 of 1000\n",
      "\n",
      "Epoch 168/1000\n",
      "----------\n",
      "train loss: 1.778659\n",
      "valid loss: 9.257807\n",
      "0.09333563835785168\n",
      "Early stopping count at 149 of 1000\n",
      "\n",
      "Epoch 169/1000\n",
      "----------\n",
      "train loss: 1.755909\n",
      "valid loss: 72.106237\n",
      "0.09240228197427315\n",
      "Early stopping count at 150 of 1000\n",
      "\n",
      "Epoch 170/1000\n",
      "----------\n",
      "train loss: 1.726825\n",
      "valid loss: 16.114248\n",
      "0.09147825915453042\n",
      "Early stopping count at 151 of 1000\n",
      "\n",
      "Epoch 171/1000\n",
      "----------\n",
      "train loss: 1.775323\n",
      "valid loss: 88.133470\n",
      "0.09056347656298512\n",
      "Early stopping count at 152 of 1000\n",
      "\n",
      "Epoch 172/1000\n",
      "----------\n",
      "train loss: 1.752253\n",
      "valid loss: 53.615655\n",
      "0.08965784179735527\n",
      "Early stopping count at 153 of 1000\n",
      "\n",
      "Epoch 173/1000\n",
      "----------\n",
      "train loss: 1.780186\n",
      "valid loss: 106.473572\n",
      "0.08876126337938171\n",
      "Early stopping count at 154 of 1000\n",
      "\n",
      "Epoch 174/1000\n",
      "----------\n",
      "train loss: 1.764319\n",
      "valid loss: 80.973933\n",
      "0.0878736507455879\n",
      "Early stopping count at 155 of 1000\n",
      "\n",
      "Epoch 175/1000\n",
      "----------\n",
      "train loss: 1.728443\n",
      "valid loss: 15.263098\n",
      "0.086994914238132\n",
      "Early stopping count at 156 of 1000\n",
      "\n",
      "Epoch 176/1000\n",
      "----------\n",
      "train loss: 1.809787\n",
      "valid loss: 109.854081\n",
      "0.0861249650957507\n",
      "Early stopping count at 157 of 1000\n",
      "\n",
      "Epoch 177/1000\n",
      "----------\n",
      "train loss: 1.759138\n",
      "valid loss: 157.994237\n",
      "0.08526371544479318\n",
      "Early stopping count at 158 of 1000\n",
      "\n",
      "Epoch 178/1000\n",
      "----------\n",
      "train loss: 1.748462\n",
      "valid loss: 15.530907\n",
      "0.08441107829034525\n",
      "Early stopping count at 159 of 1000\n",
      "\n",
      "Epoch 179/1000\n",
      "----------\n",
      "train loss: 1.770495\n",
      "valid loss: 92.123340\n",
      "0.0835669675074418\n",
      "Early stopping count at 160 of 1000\n",
      "\n",
      "Epoch 180/1000\n",
      "----------\n",
      "train loss: 1.791818\n",
      "valid loss: 97.820604\n",
      "0.08273129783236738\n",
      "Early stopping count at 161 of 1000\n",
      "\n",
      "Epoch 181/1000\n",
      "----------\n",
      "train loss: 1.828455\n",
      "valid loss: 34.194282\n",
      "0.08190398485404371\n",
      "Early stopping count at 162 of 1000\n",
      "\n",
      "Epoch 182/1000\n",
      "----------\n",
      "train loss: 1.770150\n",
      "valid loss: 75.163835\n",
      "0.08108494500550327\n",
      "Early stopping count at 163 of 1000\n",
      "\n",
      "Epoch 183/1000\n",
      "----------\n",
      "train loss: 1.754594\n",
      "valid loss: 101.003400\n",
      "0.08027409555544823\n",
      "Early stopping count at 164 of 1000\n",
      "\n",
      "Epoch 184/1000\n",
      "----------\n",
      "train loss: 1.751591\n",
      "valid loss: 173.500865\n",
      "0.07947135459989375\n",
      "Early stopping count at 165 of 1000\n",
      "\n",
      "Epoch 185/1000\n",
      "----------\n",
      "train loss: 1.759713\n",
      "valid loss: 193.361188\n",
      "0.07867664105389481\n",
      "Early stopping count at 166 of 1000\n",
      "\n",
      "Epoch 186/1000\n",
      "----------\n",
      "train loss: 1.751077\n",
      "valid loss: 145.450624\n",
      "0.07788987464335587\n",
      "Early stopping count at 167 of 1000\n",
      "\n",
      "Epoch 187/1000\n",
      "----------\n",
      "train loss: 1.853395\n",
      "valid loss: 106.014477\n",
      "0.0771109758969223\n",
      "Early stopping count at 168 of 1000\n",
      "\n",
      "Epoch 188/1000\n",
      "----------\n",
      "train loss: 1.757413\n",
      "valid loss: 127.360522\n",
      "0.07633986613795309\n",
      "Early stopping count at 169 of 1000\n",
      "\n",
      "Epoch 189/1000\n",
      "----------\n",
      "train loss: 1.823771\n",
      "valid loss: 74.171106\n",
      "0.07557646747657355\n",
      "Early stopping count at 170 of 1000\n",
      "\n",
      "Epoch 190/1000\n",
      "----------\n",
      "train loss: 1.803782\n",
      "valid loss: 117.555580\n",
      "0.07482070280180782\n",
      "Early stopping count at 171 of 1000\n",
      "\n",
      "Epoch 191/1000\n",
      "----------\n",
      "train loss: 1.737326\n",
      "valid loss: 130.181181\n",
      "0.07407249577378973\n",
      "Early stopping count at 172 of 1000\n",
      "\n",
      "Epoch 192/1000\n",
      "----------\n",
      "train loss: 1.810347\n",
      "valid loss: 88.026463\n",
      "0.07333177081605184\n",
      "Early stopping count at 173 of 1000\n",
      "\n",
      "Epoch 193/1000\n",
      "----------\n",
      "train loss: 1.937158\n",
      "valid loss: 112.420889\n",
      "0.07259845310789131\n",
      "Early stopping count at 174 of 1000\n",
      "\n",
      "Epoch 194/1000\n",
      "----------\n",
      "train loss: 1.964253\n",
      "valid loss: 521.427477\n",
      "0.0718724685768124\n",
      "Early stopping count at 175 of 1000\n",
      "\n",
      "Epoch 195/1000\n",
      "----------\n",
      "train loss: 1.918015\n",
      "valid loss: 201.158745\n",
      "0.07115374389104429\n",
      "Early stopping count at 176 of 1000\n",
      "\n",
      "Epoch 196/1000\n",
      "----------\n",
      "train loss: 1.998850\n",
      "valid loss: 37.997905\n",
      "0.07044220645213384\n",
      "Early stopping count at 177 of 1000\n",
      "\n",
      "Epoch 197/1000\n",
      "----------\n",
      "train loss: 1.897772\n",
      "valid loss: 38.751213\n",
      "0.0697377843876125\n",
      "Early stopping count at 178 of 1000\n",
      "\n",
      "Epoch 198/1000\n",
      "----------\n",
      "train loss: 1.888858\n",
      "valid loss: 99.880940\n",
      "0.06904040654373637\n",
      "Early stopping count at 179 of 1000\n",
      "\n",
      "Epoch 199/1000\n",
      "----------\n",
      "train loss: 1.857560\n",
      "valid loss: 194.957639\n",
      "0.068350002478299\n",
      "Early stopping count at 180 of 1000\n",
      "\n",
      "Epoch 200/1000\n",
      "----------\n",
      "train loss: 1.782022\n",
      "valid loss: 195.243469\n",
      "0.06766650245351602\n",
      "Early stopping count at 181 of 1000\n",
      "\n",
      "Epoch 201/1000\n",
      "----------\n",
      "train loss: 1.814854\n",
      "valid loss: 21.586645\n",
      "0.06698983742898086\n",
      "Early stopping count at 182 of 1000\n",
      "\n",
      "Epoch 202/1000\n",
      "----------\n",
      "train loss: 1.850204\n",
      "valid loss: 292.840318\n",
      "0.06631993905469105\n",
      "Early stopping count at 183 of 1000\n",
      "\n",
      "Epoch 203/1000\n",
      "----------\n",
      "train loss: 2.022933\n",
      "valid loss: 58.493119\n",
      "0.06565673966414413\n",
      "Early stopping count at 184 of 1000\n",
      "\n",
      "Epoch 204/1000\n",
      "----------\n",
      "train loss: 1.752778\n",
      "valid loss: 102.832456\n",
      "0.0650001722675027\n",
      "Early stopping count at 185 of 1000\n",
      "\n",
      "Epoch 205/1000\n",
      "----------\n",
      "train loss: 1.750797\n",
      "valid loss: 245.619236\n",
      "0.06435017054482767\n",
      "Early stopping count at 186 of 1000\n",
      "\n",
      "Epoch 206/1000\n",
      "----------\n",
      "train loss: 1.789252\n",
      "valid loss: 61.850525\n",
      "0.0637066688393794\n",
      "Early stopping count at 187 of 1000\n",
      "\n",
      "Epoch 207/1000\n",
      "----------\n",
      "train loss: 1.785202\n",
      "valid loss: 509.592298\n",
      "0.06306960215098559\n",
      "Early stopping count at 188 of 1000\n",
      "\n",
      "Epoch 208/1000\n",
      "----------\n",
      "train loss: 1.808905\n",
      "valid loss: 375.178330\n",
      "0.06243890612947574\n",
      "Early stopping count at 189 of 1000\n",
      "\n",
      "Epoch 209/1000\n",
      "----------\n",
      "train loss: 2.075612\n",
      "valid loss: 410.922472\n",
      "0.06181451706818098\n",
      "Early stopping count at 190 of 1000\n",
      "\n",
      "Epoch 210/1000\n",
      "----------\n",
      "train loss: 1.816328\n",
      "valid loss: 382.775683\n",
      "0.06119637189749917\n",
      "Early stopping count at 191 of 1000\n",
      "\n",
      "Epoch 211/1000\n",
      "----------\n",
      "train loss: 1.813975\n",
      "valid loss: 1754.000360\n",
      "0.060584408178524174\n",
      "Early stopping count at 192 of 1000\n",
      "\n",
      "Epoch 212/1000\n",
      "----------\n",
      "train loss: 1.863292\n",
      "valid loss: 614.778919\n",
      "0.059978564096738934\n",
      "Early stopping count at 193 of 1000\n",
      "\n",
      "Epoch 213/1000\n",
      "----------\n",
      "train loss: 1.963960\n",
      "valid loss: 570.576195\n",
      "0.059378778455771546\n",
      "Early stopping count at 194 of 1000\n",
      "\n",
      "Epoch 214/1000\n",
      "----------\n",
      "train loss: 2.237252\n",
      "valid loss: 611.264277\n",
      "0.05878499067121383\n",
      "Early stopping count at 195 of 1000\n",
      "\n",
      "Epoch 215/1000\n",
      "----------\n",
      "train loss: 1.876758\n",
      "valid loss: 824.620779\n",
      "0.05819714076450169\n",
      "Early stopping count at 196 of 1000\n",
      "\n",
      "Epoch 216/1000\n",
      "----------\n",
      "train loss: 1.955618\n",
      "valid loss: 877.870112\n",
      "0.05761516935685667\n",
      "Early stopping count at 197 of 1000\n",
      "\n",
      "Epoch 217/1000\n",
      "----------\n",
      "train loss: 2.035947\n",
      "valid loss: 719.500522\n",
      "0.0570390176632881\n",
      "Early stopping count at 198 of 1000\n",
      "\n",
      "Epoch 218/1000\n",
      "----------\n",
      "train loss: 1.760723\n",
      "valid loss: 158.511635\n",
      "0.05646862748665522\n",
      "Early stopping count at 199 of 1000\n",
      "\n",
      "Epoch 219/1000\n",
      "----------\n",
      "train loss: 1.859734\n",
      "valid loss: 457.009624\n",
      "0.05590394121178867\n",
      "Early stopping count at 200 of 1000\n",
      "\n",
      "Epoch 220/1000\n",
      "----------\n",
      "train loss: 1.934020\n",
      "valid loss: 823.329998\n",
      "0.05534490179967078\n",
      "Early stopping count at 201 of 1000\n",
      "\n",
      "Epoch 221/1000\n",
      "----------\n",
      "train loss: 1.809030\n",
      "valid loss: 1467.738126\n",
      "0.05479145278167408\n",
      "Early stopping count at 202 of 1000\n",
      "\n",
      "Epoch 222/1000\n",
      "----------\n",
      "train loss: 1.942902\n",
      "valid loss: 717.513999\n",
      "0.05424353825385733\n",
      "Early stopping count at 203 of 1000\n",
      "\n",
      "Epoch 223/1000\n",
      "----------\n",
      "train loss: 1.904481\n",
      "valid loss: 1071.848057\n",
      "0.05370110287131876\n",
      "Early stopping count at 204 of 1000\n",
      "\n",
      "Epoch 224/1000\n",
      "----------\n",
      "train loss: 1.823502\n",
      "valid loss: 1213.422866\n",
      "0.05316409184260557\n",
      "Early stopping count at 205 of 1000\n",
      "\n",
      "Epoch 225/1000\n",
      "----------\n",
      "train loss: 1.803168\n",
      "valid loss: 1196.629313\n",
      "0.052632450924179515\n",
      "Early stopping count at 206 of 1000\n",
      "\n",
      "Epoch 226/1000\n",
      "----------\n",
      "train loss: 1.887400\n",
      "valid loss: 993.301512\n",
      "0.05210612641493772\n",
      "Early stopping count at 207 of 1000\n",
      "\n",
      "Epoch 227/1000\n",
      "----------\n",
      "train loss: 1.893005\n",
      "valid loss: 391.116706\n",
      "0.051585065150788346\n",
      "Early stopping count at 208 of 1000\n",
      "\n",
      "Epoch 228/1000\n",
      "----------\n",
      "train loss: 1.947061\n",
      "valid loss: 3679.073857\n",
      "0.05106921449928046\n",
      "Early stopping count at 209 of 1000\n",
      "\n",
      "Epoch 229/1000\n",
      "----------\n",
      "train loss: 1.902741\n",
      "valid loss: 2157.389136\n",
      "0.050558522354287656\n",
      "Early stopping count at 210 of 1000\n",
      "\n",
      "Epoch 230/1000\n",
      "----------\n",
      "train loss: 1.940183\n",
      "valid loss: 1251.625973\n",
      "0.050052937130744775\n",
      "Early stopping count at 211 of 1000\n",
      "\n",
      "Epoch 231/1000\n",
      "----------\n",
      "train loss: 2.086131\n",
      "valid loss: 891.145372\n",
      "0.04955240775943733\n",
      "Early stopping count at 212 of 1000\n",
      "\n",
      "Epoch 232/1000\n",
      "----------\n",
      "train loss: 5.784543\n",
      "valid loss: 2605.565273\n",
      "0.04905688368184295\n",
      "Early stopping count at 213 of 1000\n",
      "\n",
      "Epoch 233/1000\n",
      "----------\n",
      "train loss: 1.814092\n",
      "valid loss: 1221.002612\n",
      "0.04856631484502452\n",
      "Early stopping count at 214 of 1000\n",
      "\n",
      "Epoch 234/1000\n",
      "----------\n",
      "train loss: 1.893298\n",
      "valid loss: 1139.154203\n",
      "0.04808065169657428\n",
      "Early stopping count at 215 of 1000\n",
      "\n",
      "Epoch 235/1000\n",
      "----------\n",
      "train loss: 1.893968\n",
      "valid loss: 1275.488760\n",
      "0.04759984517960854\n",
      "Early stopping count at 216 of 1000\n",
      "\n",
      "Epoch 236/1000\n",
      "----------\n",
      "train loss: 1.791730\n",
      "valid loss: 6388.837256\n",
      "0.04712384672781245\n",
      "Early stopping count at 217 of 1000\n",
      "\n",
      "Epoch 237/1000\n",
      "----------\n",
      "train loss: 1.887587\n",
      "valid loss: 2792.593550\n",
      "0.04665260826053433\n",
      "Early stopping count at 218 of 1000\n",
      "\n",
      "Epoch 238/1000\n",
      "----------\n",
      "train loss: 1.941671\n",
      "valid loss: 2077.577494\n",
      "0.04618608217792898\n",
      "Early stopping count at 219 of 1000\n",
      "\n",
      "Epoch 239/1000\n",
      "----------\n",
      "train loss: 2.004107\n",
      "valid loss: 1555.468365\n",
      "0.04572422135614969\n",
      "Early stopping count at 220 of 1000\n",
      "\n",
      "Epoch 240/1000\n",
      "----------\n",
      "train loss: 1.944173\n",
      "valid loss: 3753.805013\n",
      "0.0452669791425882\n",
      "Early stopping count at 221 of 1000\n",
      "\n",
      "Epoch 241/1000\n",
      "----------\n",
      "train loss: 1.930671\n",
      "valid loss: 3276.889387\n",
      "0.04481430935116231\n",
      "Early stopping count at 222 of 1000\n",
      "\n",
      "Epoch 242/1000\n",
      "----------\n",
      "train loss: 3.765452\n",
      "valid loss: 2662.684720\n",
      "0.04436616625765069\n",
      "Early stopping count at 223 of 1000\n",
      "\n",
      "Epoch 243/1000\n",
      "----------\n",
      "train loss: 2.192949\n",
      "valid loss: 6892.136799\n",
      "0.04392250459507418\n",
      "Early stopping count at 224 of 1000\n",
      "\n",
      "Epoch 244/1000\n",
      "----------\n",
      "train loss: 1.870826\n",
      "valid loss: 6849.554013\n",
      "0.04348327954912344\n",
      "Early stopping count at 225 of 1000\n",
      "\n",
      "Epoch 245/1000\n",
      "----------\n",
      "train loss: 1.877425\n",
      "valid loss: 895.101387\n",
      "0.0430484467536322\n",
      "Early stopping count at 226 of 1000\n",
      "\n",
      "Epoch 246/1000\n",
      "----------\n",
      "train loss: 2.041354\n",
      "valid loss: 4857.374137\n",
      "0.04261796228609588\n",
      "Early stopping count at 227 of 1000\n",
      "\n",
      "Epoch 247/1000\n",
      "----------\n",
      "train loss: 2.068525\n",
      "valid loss: 427.089937\n",
      "0.04219178266323492\n",
      "Early stopping count at 228 of 1000\n",
      "\n",
      "Epoch 248/1000\n",
      "----------\n",
      "train loss: 1.852317\n",
      "valid loss: 5671.271418\n",
      "0.041769864836602576\n",
      "Early stopping count at 229 of 1000\n",
      "\n",
      "Epoch 249/1000\n",
      "----------\n",
      "train loss: 1.913802\n",
      "valid loss: 804.156773\n",
      "0.04135216618823655\n",
      "Early stopping count at 230 of 1000\n",
      "\n",
      "Epoch 250/1000\n",
      "----------\n",
      "train loss: 1.978658\n",
      "valid loss: 5444.652364\n",
      "0.04093864452635418\n",
      "Early stopping count at 231 of 1000\n",
      "\n",
      "Epoch 251/1000\n",
      "----------\n",
      "train loss: 13.196410\n",
      "valid loss: 5905.859773\n",
      "0.04052925808109064\n",
      "Early stopping count at 232 of 1000\n",
      "\n",
      "Epoch 252/1000\n",
      "----------\n",
      "train loss: 2.035510\n",
      "valid loss: 1181.642360\n",
      "0.04012396550027973\n",
      "Early stopping count at 233 of 1000\n",
      "\n",
      "Epoch 253/1000\n",
      "----------\n",
      "train loss: 2.077922\n",
      "valid loss: 1429.502784\n",
      "0.03972272584527693\n",
      "Early stopping count at 234 of 1000\n",
      "\n",
      "Epoch 254/1000\n",
      "----------\n",
      "train loss: 2.061072\n",
      "valid loss: 2877.638888\n",
      "0.039325498586824166\n",
      "Early stopping count at 235 of 1000\n",
      "\n",
      "Epoch 255/1000\n",
      "----------\n",
      "train loss: 1.964732\n",
      "valid loss: 4028.749595\n",
      "0.03893224360095592\n",
      "Early stopping count at 236 of 1000\n",
      "\n",
      "Epoch 256/1000\n",
      "----------\n",
      "train loss: 1.942926\n",
      "valid loss: 6457.295849\n",
      "0.03854292116494636\n",
      "Early stopping count at 237 of 1000\n",
      "\n",
      "Epoch 257/1000\n",
      "----------\n",
      "train loss: 1.866993\n",
      "valid loss: 2597.544061\n",
      "0.0381574919532969\n",
      "Early stopping count at 238 of 1000\n",
      "\n",
      "Epoch 258/1000\n",
      "----------\n",
      "train loss: 2093.626742\n",
      "valid loss: 12442.346915\n",
      "0.03777591703376393\n",
      "Early stopping count at 239 of 1000\n",
      "\n",
      "Epoch 259/1000\n",
      "----------\n",
      "train loss: 2.159582\n",
      "valid loss: 556.945688\n",
      "0.03739815786342629\n",
      "Early stopping count at 240 of 1000\n",
      "\n",
      "Epoch 260/1000\n",
      "----------\n",
      "train loss: 1.967149\n",
      "valid loss: 5930.906653\n",
      "0.03702417628479203\n",
      "Early stopping count at 241 of 1000\n",
      "\n",
      "Epoch 261/1000\n",
      "----------\n",
      "train loss: 2.099629\n",
      "valid loss: 8956.818763\n",
      "0.036653934521944105\n",
      "Early stopping count at 242 of 1000\n",
      "\n",
      "Epoch 262/1000\n",
      "----------\n",
      "train loss: 7.475724\n",
      "valid loss: 4716.178814\n",
      "0.03628739517672466\n",
      "Early stopping count at 243 of 1000\n",
      "\n",
      "Epoch 263/1000\n",
      "----------\n",
      "train loss: 2.254157\n",
      "valid loss: 9410.118714\n",
      "0.035924521224957415\n",
      "Early stopping count at 244 of 1000\n",
      "\n",
      "Epoch 264/1000\n",
      "----------\n",
      "train loss: 2.018680\n",
      "valid loss: 12097.305115\n",
      "0.03556527601270784\n",
      "Early stopping count at 245 of 1000\n",
      "\n",
      "Epoch 265/1000\n",
      "----------\n",
      "train loss: 2.183731\n",
      "valid loss: 3271.814245\n",
      "0.035209623252580764\n",
      "Early stopping count at 246 of 1000\n",
      "\n",
      "Epoch 266/1000\n",
      "----------\n",
      "train loss: 1.979859\n",
      "valid loss: 11007.424096\n",
      "0.03485752702005496\n",
      "Early stopping count at 247 of 1000\n",
      "\n",
      "Epoch 267/1000\n",
      "----------\n",
      "train loss: 1.933719\n",
      "valid loss: 13701.461625\n",
      "0.034508951749854404\n",
      "Early stopping count at 248 of 1000\n",
      "\n",
      "Epoch 268/1000\n",
      "----------\n",
      "train loss: 2.225993\n",
      "valid loss: 19091.959038\n",
      "0.03416386223235586\n",
      "Early stopping count at 249 of 1000\n",
      "\n",
      "Epoch 269/1000\n",
      "----------\n",
      "train loss: 2.023323\n",
      "valid loss: 15271.801329\n",
      "0.0338222236100323\n",
      "Early stopping count at 250 of 1000\n",
      "\n",
      "Epoch 270/1000\n",
      "----------\n",
      "train loss: 2.084206\n",
      "valid loss: 3794.837078\n",
      "0.03348400137393198\n",
      "Early stopping count at 251 of 1000\n",
      "\n",
      "Epoch 271/1000\n",
      "----------\n",
      "train loss: 2.266240\n",
      "valid loss: 15127.177811\n",
      "0.03314916136019266\n",
      "Early stopping count at 252 of 1000\n",
      "\n",
      "Epoch 272/1000\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-05d30f77fc67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLANL_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-42cd03fec550>\u001b[0m in \u001b[0;36mLANL_train\u001b[0;34m(model, dataloaders, optimizer, criterion, num_epochs, patience, snapshot_path, model_path)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = DenseNet().to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-1, eps=1e-8, weight_decay=0.001)\n",
    "\n",
    "model, train_loss_hist, valid_loss_hist = LANL_train(model, dataloaders, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss against number of epochs')\n",
    "ax1.plot(train_loss_hist, 'r', labe l='Training Loss')\n",
    "ax1.plot(valid_loss_hist, 'g', label='Validation Loss')\n",
    "ax1.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LANL_test(model,\n",
    "              dataloaders,\n",
    "              submission_path='./submissions'):\n",
    "    predictions = {'seg_id': [], 'time_to_failure': []}\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, sample in enumerate(dataloaders['test']):\n",
    "                seg_id, X = sample['seg_id'], sample['X'].float().to(device)\n",
    "                output = model(X)\n",
    "                for i in range(X.size(0)):\n",
    "                    predictions['seg_id'].append(seg_id[i])\n",
    "                    predictions['time_to_failure'].append(output[i].item())\n",
    "    df = pd.DataFrame.from_dict(predictions)\n",
    "    datetime_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    df.to_csv(submission_path + '/submission_' + datetime_str + '.csv', index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seg_id</th>\n",
       "      <th>time_to_failure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>seg_d39131</td>\n",
       "      <td>8.859207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>seg_a975df</td>\n",
       "      <td>3.595808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>seg_634180</td>\n",
       "      <td>3.016689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>seg_ce5520</td>\n",
       "      <td>3.284222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>seg_59eb97</td>\n",
       "      <td>4.254249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       seg_id  time_to_failure\n",
       "0  seg_d39131         8.859207\n",
       "1  seg_a975df         3.595808\n",
       "2  seg_634180         3.016689\n",
       "3  seg_ce5520         3.284222\n",
       "4  seg_59eb97         4.254249"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./snapshots/snapshot_20190423-144915_2.1862827587810507.pt'))\n",
    "predictions = LANL_test(model, dataloaders)\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
