{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data wrangling imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = np.load(\"./data/train_raw.npy\")\n",
    "train_mean = 4.51945\n",
    "train_std = 10.735717\n",
    "print(len(train_raw))\n",
    "\n",
    "trainval_df = pd.read_pickle('./data/train_features.pkl')\n",
    "trainval_df = trainval_df[:-2]\n",
    "trainval_df.head()\n",
    "\n",
    "test_df = pd.read_pickle('./data/test_features.pkl')\n",
    "test_segments = np.array([seg for seg in test_df['segment'].to_numpy()])\n",
    "test_labels = test_df['target']\n",
    "test_dat = np.array([[test_segments[i], test_labels[i]] for i in range(len(test_labels))])\n",
    "print(len(test_dat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for the data loader. Extracts mean, standard deviation etc per time step.\n",
    "# Can easily be extended. Expects a two dimensional array.\n",
    "def extract_features(z):\n",
    "    return np.c_[\n",
    "        z.mean(axis=1), \n",
    "        z.min(axis=1),\n",
    "        z.max(axis=1),\n",
    "        z.std(axis=1),\n",
    "        stats.skew(z, axis=1),\n",
    "        stats.kurtosis(z, axis=1),\n",
    "        np.abs(z).max(axis=1),\n",
    "        np.abs(z).min(axis=1),\n",
    "        np.abs(z).std(axis=1),\n",
    "        np.quantile(z ,0.01, axis=1),\n",
    "        np.quantile(z ,0.05, axis=1),\n",
    "        np.quantile(z ,0.95, axis=1),\n",
    "        np.quantile(z ,0.55, axis=1)\n",
    "     ]\n",
    "    \n",
    "# For a given ending position \"last_index\", we split the last 150'000 values \n",
    "# of \"x\" into 150 pieces of length 1000 each. So n_steps * step_length should equal 150'000.\n",
    "# From each piece, a set features are extracted. This results in a feature matrix \n",
    "# of dimension (150 time steps x features).  \n",
    "def create_X(x, n_steps=150, step_length=1000, other_lens=(10, 100)):\n",
    "    # Reshaping and normalization.\n",
    "    temp = (x.reshape(n_steps, -1) - train_mean) / train_std\n",
    "    return extract_features(temp)\n",
    "\n",
    "# Query \"create_X\" to figure out the number of features\n",
    "n_features = create_X(trainval_df[\"segment\"][0]).shape[1]\n",
    "print(\"Our RNN is based on %i features\"% n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LANLDataset(data.Dataset):\n",
    "    def __init__(self, datas, n_seqs=5000 ,n_steps=150, step_length=1000, n_features=12, phase=\"train\"):\n",
    "        self.samples = torch.Tensor(n_seqs, n_steps, n_features)\n",
    "        self.targets = torch.Tensor(n_seqs)\n",
    "        if phase == \"train\":\n",
    "            # Sample n_seqs indices from the whole data\n",
    "            sampled_indices = sorted(np.random.randint(n_steps * step_length, high=len(datas), size=n_seqs))\n",
    "            for i, idx in enumerate(sampled_indices):\n",
    "                chunk = datas[idx - (n_steps * step_length) : idx]\n",
    "                self.samples[i] = torch.tensor(\n",
    "                    create_X(chunk[:,0], n_steps=n_steps, step_length=step_length)\n",
    "                )\n",
    "                self.targets[i] = torch.tensor([chunk[-1, 1]])\n",
    "        else:\n",
    "            for i, chunk in enumerate(datas):\n",
    "                self.samples[i] = torch.tensor(\n",
    "                    create_X(chunk[0], n_steps=n_steps, step_length=step_length)\n",
    "                )\n",
    "                self.targets[i] = torch.tensor([-999])\n",
    "        print(self.samples.shape)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        return self.samples[index], self.targets[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "    \n",
    "    def train_val_split(self, train_ratio, val_ratio):\n",
    "        if (train_ratio + val_ratio != 1):\n",
    "            raise Exception('Ratios should sum to one.')\n",
    "        train_length = int(train_ratio * len(self))\n",
    "        val_length = len(self) - train_length\n",
    "        splits = [train_length, val_length]\n",
    "        return data.random_split(self, splits)\n",
    "\n",
    "    \n",
    "batch_size = 32\n",
    "\n",
    "# Initialize data loaders\n",
    "trainval_data = LANLDataset(train_raw, n_seqs=5000, n_steps=150, step_length=1000, n_features=13)\n",
    "train_data, val_data = trainval_data.train_val_split(0.8, 0.2)\n",
    "test_data = LANLDataset(test_dat, n_steps=150, step_length=1000, n_features=13, phase=\"test\")\n",
    "\n",
    "datasets = {\n",
    "    \"train\": train_data, \n",
    "    \"val\": val_data,\n",
    "    \"test\": test_data\n",
    "}\n",
    "\n",
    "dl_params = {\n",
    "    \"batch_size\": batch_size,\n",
    "    \"shuffle\": False,\n",
    "    \"num_workers\": 16\n",
    "}\n",
    "dataloaders = {\n",
    "    phase: data.DataLoader(dataset, **dl_params)\n",
    "    for phase, dataset in datasets.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LANLModel(nn.Module):\n",
    "    def __init__(self, device, input_dim=1, hidden_dim=64, output_dim=1, batch_size=64, num_layers=1):\n",
    "        super(LANLModel, self).__init__()\n",
    "        self.device = device\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (\n",
    "            torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).to(self.device),\n",
    "            torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).to(self.device)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = x.unsqueeze(2)\n",
    "        rnn_out, _ = self.rnn(x, self.init_hidden())\n",
    "        out = self.fc(rnn_out[:,-1,:])\n",
    "        return out\n",
    "\n",
    "\n",
    "model_params = {\n",
    "    \"batch_size\": batch_size,\n",
    "    \"input_dim\": 13,\n",
    "    \"hidden_dim\": 48,\n",
    "    \"num_layers\": 1\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = LANLModel(device, **model_params)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "print(len(dataloaders[\"train\"]))\n",
    "\n",
    "def train():\n",
    "    model.train(mode=True)\n",
    "    for epoch in range(30):\n",
    "        print(\"Epoch \" + str(epoch))\n",
    "        train_loss = 0\n",
    "        for idx, (samples, targets) in enumerate(dataloaders[\"train\"]):\n",
    "            if idx == len(dataloaders[\"train\"]) - 1:\n",
    "                continue\n",
    "            samples, targets = samples.to(device), targets.to(device)\n",
    "            model.zero_grad()\n",
    "            outputs = model(samples)\n",
    "            #print(outputs)\n",
    "            loss = criterion(outputs.float(), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= (len(dataloaders[\"train\"]) - 1)\n",
    "        print(train_loss)\n",
    "\n",
    "train()\n",
    "torch.save(model.state_dict(), \"./model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def LANL_test(model, dataloaders, submission_path='./submissions'):\n",
    "    predictions = {'seg_id': [], 'time_to_failure': []}\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (features, _) in enumerate(dataloaders['test']):\n",
    "            features = features.to(device)\n",
    "            start, end = idx, idx + features.shape[0]\n",
    "            seg_id = test_df['seg_id'][start:end]\n",
    "            output = model(features).squeeze(0)\n",
    "            for i in range(features.shape[0]):\n",
    "                predictions['seg_id'].append(seg_id.values[i])\n",
    "                predictions['time_to_failure'].append(output[i].item())\n",
    "    df = pd.DataFrame.from_dict(predictions)\n",
    "    datetime_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    df.to_csv(submission_path + '/submission_' + datetime_str + '.csv', index=False)\n",
    "    return df\n",
    "\n",
    "LANL_test(model, dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
